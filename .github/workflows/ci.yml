name: CI-CD

on:
  push:
    branches: ["main", "test"]
  pull_request:
  workflow_dispatch:

permissions:
  contents: read
  security-events: write
  id-token: write

env:
  AWS_REGION: eu-west-3

jobs:
  build_test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_DB: notesdb
          POSTGRES_USER: notes
          POSTGRES_PASSWORD: notes_pwd
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U notes -d notesdb"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
    steps:
      - uses: actions/checkout@v4

      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "21"

      - name: Build and test appointments service
        env:
          DB_URL: jdbc:postgresql://localhost:5432/notesdb
          DB_USER: notes
          DB_PASSWORD: notes_pwd
        run: mvn -f app/pom.xml -q test package

      - name: Build and test auth service
        env:
          DB_URL: jdbc:postgresql://localhost:5432/notesdb
          DB_USER: notes
          DB_PASSWORD: notes_pwd
        run: mvn -f auth-service/pom.xml -q test package

  codeql_scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "21"
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: java,javascript
      - name: Build for CodeQL
        run: |
          mvn -f app/pom.xml -q -DskipTests package
          mvn -f auth-service/pom.xml -q -DskipTests package
      - name: Analyze with CodeQL
        uses: github/codeql-action/analyze@v3

  secret_scanning:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Scan secrets with Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  image_security:
    needs: [build_test, codeql_scan, secret_scanning]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build appointments image locally
        uses: docker/build-push-action@v6
        with:
          context: ./app
          file: ./app/Dockerfile
          push: false
          load: true
          tags: appointments-local:${{ github.sha }}
      - name: Build auth image locally
        uses: docker/build-push-action@v6
        with:
          context: ./auth-service
          file: ./auth-service/Dockerfile
          push: false
          load: true
          tags: auth-local:${{ github.sha }}
      - name: Scan appointments image with Trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: appointments-local:${{ github.sha }}
          format: table
          vuln-type: os,library
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: "1"
      - name: Scan auth image with Trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: auth-local:${{ github.sha }}
          format: table
          vuln-type: os,library
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: "1"

  deploy_precheck:
    needs: image_security
    runs-on: ubuntu-latest
    outputs:
      deploy_enabled: ${{ steps.precheck.outputs.deploy_enabled }}
      deploy_reason: ${{ steps.precheck.outputs.deploy_reason }}
    steps:
      - name: Evaluate deploy prerequisites
        id: precheck
        env:
          AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}
          TF_LOCK_TABLE: ${{ secrets.TF_LOCK_TABLE }}
          EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
          DB_PASSWORD_MAIN: ${{ secrets.DB_PASSWORD_MAIN }}
          DB_PASSWORD_TEST: ${{ secrets.DB_PASSWORD_TEST }}
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          ALLOWED_EVENT=false
          if [ "${GITHUB_EVENT_NAME}" = "push" ] && { [ "$BRANCH" = "main" ] || [ "$BRANCH" = "test" ]; }; then
            ALLOWED_EVENT=true
          elif [ "${GITHUB_EVENT_NAME}" = "pull_request" ] && { [ "$BRANCH" = "main" ] || [ "$BRANCH" = "test" ]; } && [ "${{ github.event.pull_request.head.repo.fork }}" = "false" ]; then
            ALLOWED_EVENT=true
          fi

          REQUIRED_VARS="AWS_ROLE_TO_ASSUME TF_STATE_BUCKET TF_LOCK_TABLE EKS_CLUSTER_NAME"
          if [ "$BRANCH" = "main" ]; then
            REQUIRED_VARS="$REQUIRED_VARS DB_PASSWORD_MAIN"
          elif [ "$BRANCH" = "test" ]; then
            REQUIRED_VARS="$REQUIRED_VARS DB_PASSWORD_TEST"
          fi

          MISSING=""
          for v in $REQUIRED_VARS; do
            if [ -z "${!v}" ]; then
              MISSING="$MISSING $v"
            fi
          done

          INVALID_ARN=false
          if [ -n "${AWS_ROLE_TO_ASSUME}" ]; then
            if ! [[ "${AWS_ROLE_TO_ASSUME}" =~ ^arn:aws:iam::[0-9]{12}:role/.+ ]]; then
              INVALID_ARN=true
            fi
          fi

          if [ "$ALLOWED_EVENT" = "true" ] && [ -z "$MISSING" ] && [ "$INVALID_ARN" = "false" ]; then
            echo "deploy_enabled=true" >> "$GITHUB_OUTPUT"
            echo "deploy_reason=ok" >> "$GITHUB_OUTPUT"
            echo "Deploy precheck passed."
          else
            echo "deploy_enabled=false" >> "$GITHUB_OUTPUT"
            REASON="precheck_blocked"
            echo "Deploy precheck failed or not applicable."
            if [ "$ALLOWED_EVENT" != "true" ]; then
              echo "Reason: event/branch not eligible for deploy."
              REASON="event_or_branch_not_eligible"
            fi
            if [ -n "$MISSING" ]; then
              echo "Reason: missing required secrets:$MISSING"
              REASON="missing_required_secrets:$MISSING"
            fi
            if [ "$INVALID_ARN" = "true" ]; then
              echo "Reason: AWS_ROLE_TO_ASSUME is not a valid IAM role ARN."
              REASON="invalid_aws_role_arn"
            fi
            echo "deploy_reason=$REASON" >> "$GITHUB_OUTPUT"
            if [ "$ALLOWED_EVENT" = "true" ] && { [ -n "$MISSING" ] || [ "$INVALID_ARN" = "true" ]; }; then
              echo "Failing precheck because deploy is eligible but prerequisites are invalid."
              exit 1
            fi
          fi

  terraform_infra:
    if: needs.deploy_precheck.outputs.deploy_enabled == 'true'
    needs: [image_security, deploy_precheck]
    runs-on: ubuntu-latest
    concurrency:
      group: terraform-eks-shared
      cancel-in-progress: false
    outputs:
      cluster_name: ${{ steps.tfout.outputs.cluster_name }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Ensure Terraform backend exists
        env:
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}
          TF_LOCK_TABLE: ${{ secrets.TF_LOCK_TABLE }}
        run: |
          set -euo pipefail

          if ! aws s3api head-bucket --bucket "${TF_STATE_BUCKET}" 2>/dev/null; then
            echo "Creating missing S3 backend bucket: ${TF_STATE_BUCKET}"
            aws s3api create-bucket \
              --bucket "${TF_STATE_BUCKET}" \
              --region "${AWS_REGION}" \
              --create-bucket-configuration LocationConstraint="${AWS_REGION}"
          else
            echo "S3 backend bucket exists: ${TF_STATE_BUCKET}"
          fi

          aws s3api put-bucket-versioning \
            --bucket "${TF_STATE_BUCKET}" \
            --versioning-configuration Status=Enabled

          aws s3api put-bucket-encryption \
            --bucket "${TF_STATE_BUCKET}" \
            --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'

          aws s3api put-public-access-block \
            --bucket "${TF_STATE_BUCKET}" \
            --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true

          if ! aws dynamodb describe-table --table-name "${TF_LOCK_TABLE}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            echo "Creating missing DynamoDB lock table: ${TF_LOCK_TABLE}"
            aws dynamodb create-table \
              --table-name "${TF_LOCK_TABLE}" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region "${AWS_REGION}"
            aws dynamodb wait table-exists \
              --table-name "${TF_LOCK_TABLE}" \
              --region "${AWS_REGION}"
          else
            echo "DynamoDB lock table exists: ${TF_LOCK_TABLE}"
          fi

      - name: Resolve env
        id: vars
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          if [ "$BRANCH" = "main" ]; then
            echo "env_name=main" >> $GITHUB_OUTPUT
          else
            echo "env_name=test" >> $GITHUB_OUTPUT
          fi

      - name: Terraform init
        working-directory: infra/terraform/eks
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=cloud-projet/eks/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_LOCK_TABLE }}" \
            -backend-config="encrypt=true"

      - name: Terraform validate
        working-directory: infra/terraform/eks
        run: terraform validate

      - name: Terraform plan
        working-directory: infra/terraform/eks
        run: |
          terraform plan -out=tfplan \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="environment=${{ steps.vars.outputs.env_name }}" \
            -var="cluster_name=${{ secrets.EKS_CLUSTER_NAME }}"

      - name: Terraform apply
        working-directory: infra/terraform/eks
        run: terraform apply -auto-approve tfplan

      - name: Capture terraform outputs
        id: tfout
        working-directory: infra/terraform/eks
        run: |
          echo "cluster_name=$(terraform output -raw cluster_name)" >> $GITHUB_OUTPUT

  deploy_aws_eks:
    if: needs.deploy_precheck.outputs.deploy_enabled == 'true'
    needs: [image_security, deploy_precheck, terraform_infra]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Resolve environment
        id: vars
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          if [ "$BRANCH" = "main" ]; then
            echo "namespace=demo-main" >> $GITHUB_OUTPUT
            echo "db_password=${{ secrets.DB_PASSWORD_MAIN }}" >> $GITHUB_OUTPUT
            echo "env_name=main" >> $GITHUB_OUTPUT
          else
            echo "namespace=demo-test" >> $GITHUB_OUTPUT
            echo "db_password=${{ secrets.DB_PASSWORD_TEST }}" >> $GITHUB_OUTPUT
            echo "env_name=test" >> $GITHUB_OUTPUT
          fi

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push appointments image
        uses: docker/build-push-action@v6
        with:
          context: ./app
          file: ./app/Dockerfile
          push: true
          tags: ${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:${{ github.sha }},${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:latest

      - name: Build and push auth image
        uses: docker/build-push-action@v6
        with:
          context: ./auth-service
          file: ./auth-service/Dockerfile
          push: true
          tags: ${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:${{ github.sha }},${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:latest

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ needs.terraform_infra.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      - name: Ensure namespace exists
        run: |
          kubectl get namespace ${{ steps.vars.outputs.namespace }} >/dev/null 2>&1 || kubectl create namespace ${{ steps.vars.outputs.namespace }}

      - name: Render manifests
        run: |
          rm -rf rendered
          cp -R k8s/eks rendered
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s/namespace: demo/namespace: ${{ steps.vars.outputs.namespace }}/g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__APPOINTMENTS_IMAGE__|${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:${{ github.sha }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__AUTH_IMAGE__|${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:${{ github.sha }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__SET_IN_CLUSTER__|${{ steps.vars.outputs.db_password }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__APPOINTMENTS_BASE_URL__|http://placeholder.local|g"

      - name: Apply manifests
        run: kubectl apply -k rendered

      - name: Wait ingress and patch auth base URL
        run: |
          kubectl wait --for=condition=available --timeout=300s deployment/appointments-api -n ${{ steps.vars.outputs.namespace }}
          kubectl wait --for=condition=available --timeout=300s deployment/auth-api -n ${{ steps.vars.outputs.namespace }}
          for i in $(seq 1 30); do
            ALB_HOST=$(kubectl get ingress app-ingress -n ${{ steps.vars.outputs.namespace }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || true)
            if [ -n "$ALB_HOST" ]; then break; fi
            sleep 10
          done
          if [ -z "$ALB_HOST" ]; then
            echo "ALB hostname not ready yet"
            exit 1
          fi
          kubectl patch configmap auth-config -n ${{ steps.vars.outputs.namespace }} --type merge -p "{\"data\":{\"AUTH_APP_BASE_URL\":\"http://$ALB_HOST\"}}"
          kubectl rollout restart deployment/auth-api -n ${{ steps.vars.outputs.namespace }}
          echo "Application URL: http://$ALB_HOST"
