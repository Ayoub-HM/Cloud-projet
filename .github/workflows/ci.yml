name: CI-CD

on:
  push:
    branches: ["main", "test"]
  pull_request:
  workflow_dispatch:

permissions:
  contents: read
  security-events: write
  id-token: write

env:
  AWS_REGION: eu-west-3

jobs:
  build_test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_DB: notesdb
          POSTGRES_USER: notes
          POSTGRES_PASSWORD: notes_pwd
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U notes -d notesdb"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
    steps:
      - uses: actions/checkout@v4

      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "21"
          cache: maven

      - name: Build and test appointments service
        env:
          DB_URL: jdbc:postgresql://localhost:5432/notesdb
          DB_USER: notes
          DB_PASSWORD: notes_pwd
        run: mvn -f app/pom.xml -q test

      - name: Build and test auth service
        env:
          DB_URL: jdbc:postgresql://localhost:5432/notesdb
          DB_USER: notes
          DB_PASSWORD: notes_pwd
        run: mvn -f auth-service/pom.xml -q test

  codeql_scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "21"
          cache: maven
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: java,javascript
      - name: Build for CodeQL
        run: |
          mvn -f app/pom.xml -q -DskipTests package
          mvn -f auth-service/pom.xml -q -DskipTests package
      - name: Analyze with CodeQL
        uses: github/codeql-action/analyze@v3

  secret_scanning:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Scan secrets with Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  image_security:
    needs: [build_test, codeql_scan, secret_scanning]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build appointments image locally
        uses: docker/build-push-action@v6
        with:
          context: ./app
          file: ./app/Dockerfile
          push: false
          load: true
          tags: appointments-local:${{ github.sha }}
          cache-from: type=gha,scope=appointments-local
          cache-to: type=gha,mode=max,scope=appointments-local
      - name: Build auth image locally
        uses: docker/build-push-action@v6
        with:
          context: ./auth-service
          file: ./auth-service/Dockerfile
          push: false
          load: true
          tags: auth-local:${{ github.sha }}
          cache-from: type=gha,scope=auth-local
          cache-to: type=gha,mode=max,scope=auth-local
      - name: Scan appointments image with Trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: appointments-local:${{ github.sha }}
          format: table
          vuln-type: os,library
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: "1"
      - name: Scan auth image with Trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: auth-local:${{ github.sha }}
          format: table
          vuln-type: os,library
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: "1"

  deploy_precheck:
    needs: image_security
    runs-on: ubuntu-latest
    outputs:
      deploy_enabled: ${{ steps.precheck.outputs.deploy_enabled }}
      deploy_reason: ${{ steps.precheck.outputs.deploy_reason }}
    steps:
      - name: Evaluate deploy prerequisites
        id: precheck
        env:
          AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}
          TF_LOCK_TABLE: ${{ secrets.TF_LOCK_TABLE }}
          EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
          DB_PASSWORD_MAIN: ${{ secrets.DB_PASSWORD_MAIN }}
          DB_PASSWORD_TEST: ${{ secrets.DB_PASSWORD_TEST }}
          AUTH_JWT_SECRET_MAIN: ${{ secrets.AUTH_JWT_SECRET_MAIN }}
          AUTH_JWT_SECRET_TEST: ${{ secrets.AUTH_JWT_SECRET_TEST }}
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          ALLOWED_EVENT=false
          if [ "${GITHUB_EVENT_NAME}" = "push" ] && { [ "$BRANCH" = "main" ] || [ "$BRANCH" = "test" ]; }; then
            ALLOWED_EVENT=true
          elif [ "${GITHUB_EVENT_NAME}" = "pull_request" ] && { [ "$BRANCH" = "main" ] || [ "$BRANCH" = "test" ]; } && [ "${{ github.event.pull_request.head.repo.fork }}" = "false" ]; then
            ALLOWED_EVENT=true
          fi

          REQUIRED_VARS="AWS_ROLE_TO_ASSUME TF_STATE_BUCKET TF_LOCK_TABLE EKS_CLUSTER_NAME"
          if [ "$BRANCH" = "main" ]; then
            REQUIRED_VARS="$REQUIRED_VARS DB_PASSWORD_MAIN"
          elif [ "$BRANCH" = "test" ]; then
            REQUIRED_VARS="$REQUIRED_VARS DB_PASSWORD_TEST"
          fi

          MISSING=""
          for v in $REQUIRED_VARS; do
            if [ -z "${!v}" ]; then
              MISSING="$MISSING $v"
            fi
          done

          INVALID_ARN=false
          if [ -n "${AWS_ROLE_TO_ASSUME}" ]; then
            if ! [[ "${AWS_ROLE_TO_ASSUME}" =~ ^arn:aws:iam::[0-9]{12}:role/.+ ]]; then
              INVALID_ARN=true
            fi
          fi

          INVALID_CLUSTER_NAME=false
          if [ -n "${EKS_CLUSTER_NAME}" ]; then
            EKS_CLUSTER_NAME_TRIMMED="$(echo "${EKS_CLUSTER_NAME}" | xargs)"
            if ! [[ "${EKS_CLUSTER_NAME_TRIMMED}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
              INVALID_CLUSTER_NAME=true
            fi
          fi

          JWT_SECRET_TO_CHECK=""
          if [ "$BRANCH" = "main" ]; then
            JWT_SECRET_TO_CHECK="${AUTH_JWT_SECRET_MAIN}"
          elif [ "$BRANCH" = "test" ]; then
            JWT_SECRET_TO_CHECK="${AUTH_JWT_SECRET_TEST}"
          fi

          INVALID_AUTH_JWT_SECRET=false
          if [ -n "${JWT_SECRET_TO_CHECK}" ] && [ "${#JWT_SECRET_TO_CHECK}" -lt 32 ]; then
            INVALID_AUTH_JWT_SECRET=true
          fi

          if [ "$ALLOWED_EVENT" = "true" ] && [ -z "$MISSING" ] && [ "$INVALID_ARN" = "false" ] && [ "$INVALID_CLUSTER_NAME" = "false" ] && [ "$INVALID_AUTH_JWT_SECRET" = "false" ]; then
            echo "deploy_enabled=true" >> "$GITHUB_OUTPUT"
            echo "deploy_reason=ok" >> "$GITHUB_OUTPUT"
            echo "Deploy precheck passed."
          else
            echo "deploy_enabled=false" >> "$GITHUB_OUTPUT"
            REASON="precheck_blocked"
            echo "Deploy precheck failed or not applicable."
            if [ "$ALLOWED_EVENT" != "true" ]; then
              echo "Reason: event/branch not eligible for deploy."
              REASON="event_or_branch_not_eligible"
            fi
            if [ -n "$MISSING" ]; then
              echo "Reason: missing required secrets:$MISSING"
              REASON="missing_required_secrets:$MISSING"
            fi
            if [ "$INVALID_ARN" = "true" ]; then
              echo "Reason: AWS_ROLE_TO_ASSUME is not a valid IAM role ARN."
              REASON="invalid_aws_role_arn"
            fi
            if [ "$INVALID_CLUSTER_NAME" = "true" ]; then
              echo "Reason: EKS_CLUSTER_NAME is invalid (allowed: letters, numbers, '_' and '-', no spaces)."
              REASON="invalid_eks_cluster_name"
            fi
            if [ "$INVALID_AUTH_JWT_SECRET" = "true" ]; then
              echo "Reason: AUTH_JWT_SECRET_<ENV> is too short (minimum 32 characters)."
              REASON="invalid_auth_jwt_secret"
            fi
            echo "deploy_reason=$REASON" >> "$GITHUB_OUTPUT"
            if [ "$ALLOWED_EVENT" = "true" ] && { [ -n "$MISSING" ] || [ "$INVALID_ARN" = "true" ] || [ "$INVALID_CLUSTER_NAME" = "true" ] || [ "$INVALID_AUTH_JWT_SECRET" = "true" ]; }; then
              echo "Failing precheck because deploy is eligible but prerequisites are invalid."
              exit 1
            fi
          fi

  terraform_infra:
    if: needs.deploy_precheck.outputs.deploy_enabled == 'true'
    needs: [image_security, deploy_precheck]
    runs-on: ubuntu-latest
    concurrency:
      group: terraform-eks-shared
      cancel-in-progress: false
    outputs:
      cluster_name: ${{ steps.tfout.outputs.cluster_name }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Normalize backend config
        id: backend
        env:
          TF_STATE_BUCKET_RAW: ${{ secrets.TF_STATE_BUCKET }}
          TF_LOCK_TABLE_RAW: ${{ secrets.TF_LOCK_TABLE }}
        run: |
          set -euo pipefail
          TF_STATE_BUCKET="$(echo "${TF_STATE_BUCKET_RAW}" | xargs)"
          TF_LOCK_TABLE="$(echo "${TF_LOCK_TABLE_RAW}" | xargs)"

          if [ -z "${TF_STATE_BUCKET}" ] || [ -z "${TF_LOCK_TABLE}" ]; then
            echo "TF_STATE_BUCKET and TF_LOCK_TABLE must be non-empty."
            exit 1
          fi

          if ! [[ "${TF_STATE_BUCKET}" =~ ^[a-z0-9][a-z0-9.-]{1,61}[a-z0-9]$ ]]; then
            echo "Invalid TF_STATE_BUCKET format: must be a valid S3 bucket name."
            exit 1
          fi

          if ! [[ "${TF_LOCK_TABLE}" =~ ^[A-Za-z0-9_.-]{3,255}$ ]]; then
            echo "Invalid TF_LOCK_TABLE format: must match DynamoDB table naming rules."
            exit 1
          fi

          echo "tf_state_bucket=${TF_STATE_BUCKET}" >> "$GITHUB_OUTPUT"
          echo "tf_lock_table=${TF_LOCK_TABLE}" >> "$GITHUB_OUTPUT"

      - name: Ensure Terraform backend exists
        env:
          TF_STATE_BUCKET: ${{ steps.backend.outputs.tf_state_bucket }}
          TF_LOCK_TABLE: ${{ steps.backend.outputs.tf_lock_table }}
        run: |
          set -euo pipefail

          if ! aws s3api head-bucket --bucket "${TF_STATE_BUCKET}" 2>/dev/null; then
            echo "Creating missing S3 backend bucket: ${TF_STATE_BUCKET}"
            aws s3api create-bucket \
              --bucket "${TF_STATE_BUCKET}" \
              --region "${AWS_REGION}" \
              --create-bucket-configuration LocationConstraint="${AWS_REGION}"
            aws s3api wait bucket-exists --bucket "${TF_STATE_BUCKET}"
          else
            echo "S3 backend bucket exists: ${TF_STATE_BUCKET}"
          fi

          aws s3api head-bucket --bucket "${TF_STATE_BUCKET}" >/dev/null

          aws s3api put-bucket-versioning \
            --bucket "${TF_STATE_BUCKET}" \
            --versioning-configuration Status=Enabled

          aws s3api put-bucket-encryption \
            --bucket "${TF_STATE_BUCKET}" \
            --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'

          aws s3api put-public-access-block \
            --bucket "${TF_STATE_BUCKET}" \
            --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true

          if ! aws dynamodb describe-table --table-name "${TF_LOCK_TABLE}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            echo "Creating missing DynamoDB lock table: ${TF_LOCK_TABLE}"
            aws dynamodb create-table \
              --table-name "${TF_LOCK_TABLE}" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region "${AWS_REGION}"
            aws dynamodb wait table-exists \
              --table-name "${TF_LOCK_TABLE}" \
              --region "${AWS_REGION}"
          else
            echo "DynamoDB lock table exists: ${TF_LOCK_TABLE}"
          fi

      - name: Resolve env
        id: vars
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          if [ "$BRANCH" = "main" ]; then
            echo "env_name=main" >> $GITHUB_OUTPUT
          else
            echo "env_name=test" >> $GITHUB_OUTPUT
          fi

      - name: Resolve nodegroup sizing
        id: sizing
        run: |
          if [ "${{ steps.vars.outputs.env_name }}" = "main" ]; then
            echo "node_desired_size=2" >> "$GITHUB_OUTPUT"
            echo "node_min_size=2" >> "$GITHUB_OUTPUT"
            echo "node_max_size=4" >> "$GITHUB_OUTPUT"
          else
            echo "node_desired_size=1" >> "$GITHUB_OUTPUT"
            echo "node_min_size=1" >> "$GITHUB_OUTPUT"
            echo "node_max_size=2" >> "$GITHUB_OUTPUT"
          fi

      - name: Normalize cluster name
        id: cluster
        env:
          EKS_CLUSTER_NAME_RAW: ${{ secrets.EKS_CLUSTER_NAME }}
        run: |
          set -euo pipefail
          EKS_CLUSTER_NAME="$(echo "${EKS_CLUSTER_NAME_RAW}" | xargs)"
          if [ -z "${EKS_CLUSTER_NAME}" ]; then
            echo "EKS_CLUSTER_NAME must be non-empty."
            exit 1
          fi
          if ! [[ "${EKS_CLUSTER_NAME}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
            echo "EKS_CLUSTER_NAME is invalid. Allowed pattern: ^[0-9A-Za-z][0-9A-Za-z_-]*$"
            exit 1
          fi
          echo "cluster_name=${EKS_CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

      - name: Terraform init
        working-directory: infra/terraform/eks
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ steps.backend.outputs.tf_state_bucket }}" \
            -backend-config="key=cloud-projet/eks/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ steps.backend.outputs.tf_lock_table }}" \
            -backend-config="encrypt=true"

      - name: Terraform validate
        working-directory: infra/terraform/eks
        run: terraform validate

      - name: Import existing AWS resources into Terraform state
        working-directory: infra/terraform/eks
        run: |
          set -euo pipefail

          ENV_NAME="${{ steps.vars.outputs.env_name }}"
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          NAME_PREFIX="cloud-projet-${ENV_NAME}"
          CLUSTER_ROLE_NAME="${NAME_PREFIX}-eks-cluster-role"
          NODES_ROLE_NAME="${NAME_PREFIX}-eks-nodes-role"
          APPOINTMENTS_REPO="${NAME_PREFIX}/appointments-api"
          AUTH_REPO="${NAME_PREFIX}/auth-api"
          NODEGROUP_NAME="${NAME_PREFIX}-ng"

          import_if_missing() {
            local addr="$1"
            local id="$2"
            if terraform state show "$addr" >/dev/null 2>&1; then
              echo "State already has ${addr}"
            else
              echo "Importing ${addr} <- ${id}"
              terraform import "$addr" "$id"
            fi
          }

          role_has_policy() {
            local role_name="$1"
            local policy_arn="$2"
            aws iam list-attached-role-policies \
              --role-name "${role_name}" \
              --query "AttachedPolicies[?PolicyArn=='${policy_arn}'].PolicyArn" \
              --output text | grep -Fxq "${policy_arn}"
          }

          if aws iam get-role --role-name "${CLUSTER_ROLE_NAME}" >/dev/null 2>&1; then
            import_if_missing aws_iam_role.eks_cluster "${CLUSTER_ROLE_NAME}"
            if role_has_policy "${CLUSTER_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"; then
              import_if_missing aws_iam_role_policy_attachment.eks_cluster "${CLUSTER_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
            fi
          fi

          if aws iam get-role --role-name "${NODES_ROLE_NAME}" >/dev/null 2>&1; then
            import_if_missing aws_iam_role.eks_nodes "${NODES_ROLE_NAME}"
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"; then
              import_if_missing aws_iam_role_policy_attachment.node_worker_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
            fi
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"; then
              import_if_missing aws_iam_role_policy_attachment.node_cni_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
            fi
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"; then
              import_if_missing aws_iam_role_policy_attachment.node_ecr_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
            fi
          fi

          if aws ecr describe-repositories --repository-names "${APPOINTMENTS_REPO}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_ecr_repository.appointments "${APPOINTMENTS_REPO}"
          fi

          if aws ecr describe-repositories --repository-names "${AUTH_REPO}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_ecr_repository.auth "${AUTH_REPO}"
          fi

          if aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_eks_cluster.this "${CLUSTER_NAME}"
          fi

          if aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_eks_node_group.default "${CLUSTER_NAME}:${NODEGROUP_NAME}"
          fi

      - name: Terraform plan
        working-directory: infra/terraform/eks
        run: |
          terraform plan -out=tfplan \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="environment=${{ steps.vars.outputs.env_name }}" \
            -var="cluster_name=${{ steps.cluster.outputs.cluster_name }}" \
            -var="node_desired_size=${{ steps.sizing.outputs.node_desired_size }}" \
            -var="node_min_size=${{ steps.sizing.outputs.node_min_size }}" \
            -var="node_max_size=${{ steps.sizing.outputs.node_max_size }}"

      - name: Terraform apply
        working-directory: infra/terraform/eks
        run: terraform apply -auto-approve tfplan

      - name: Capture terraform outputs
        id: tfout
        working-directory: infra/terraform/eks
        run: |
          set -euo pipefail
          CLUSTER_NAME="$(terraform output -raw cluster_name | xargs)"
          if [ -z "${CLUSTER_NAME}" ]; then
            echo "Terraform output 'cluster_name' is empty."
            exit 1
          fi
          echo "cluster_name=${CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

  deploy_aws_eks:
    if: needs.deploy_precheck.outputs.deploy_enabled == 'true'
    needs: [image_security, deploy_precheck, terraform_infra]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Resolve environment
        id: vars
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          if [ "$BRANCH" = "main" ]; then
            echo "namespace=demo-main" >> $GITHUB_OUTPUT
            DB_PASSWORD="${{ secrets.DB_PASSWORD_MAIN }}"
            JWT_SECRET="${{ secrets.AUTH_JWT_SECRET_MAIN }}"
            echo "env_name=main" >> $GITHUB_OUTPUT
          else
            echo "namespace=demo-test" >> $GITHUB_OUTPUT
            DB_PASSWORD="${{ secrets.DB_PASSWORD_TEST }}"
            JWT_SECRET="${{ secrets.AUTH_JWT_SECRET_TEST }}"
            echo "env_name=test" >> $GITHUB_OUTPUT
          fi
          if [ -z "${JWT_SECRET}" ]; then
            JWT_SECRET="${DB_PASSWORD}-jwt"
          fi
          if [ "${#JWT_SECRET}" -lt 32 ]; then
            JWT_SECRET="${JWT_SECRET}0123456789abcdef0123456789abcdef"
          fi
          echo "db_password=${DB_PASSWORD}" >> $GITHUB_OUTPUT
          echo "auth_jwt_secret=${JWT_SECRET}" >> $GITHUB_OUTPUT

      - name: Resolve cluster name
        id: cluster
        env:
          TF_CLUSTER_NAME_RAW: ${{ needs.terraform_infra.outputs.cluster_name }}
          EKS_CLUSTER_NAME_RAW: ${{ secrets.EKS_CLUSTER_NAME }}
        run: |
          set -euo pipefail
          TF_CLUSTER_NAME="$(echo "${TF_CLUSTER_NAME_RAW}" | xargs)"
          EKS_CLUSTER_NAME="$(echo "${EKS_CLUSTER_NAME_RAW}" | xargs)"

          CLUSTER_NAME="${TF_CLUSTER_NAME}"
          if [ -z "${CLUSTER_NAME}" ]; then
            CLUSTER_NAME="${EKS_CLUSTER_NAME}"
          fi

          if [ -z "${CLUSTER_NAME}" ]; then
            echo "Cluster name is empty from both Terraform output and EKS_CLUSTER_NAME secret."
            exit 1
          fi

          if ! [[ "${CLUSTER_NAME}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
            echo "Cluster name is invalid. Allowed pattern: ^[0-9A-Za-z][0-9A-Za-z_-]*$"
            exit 1
          fi

          echo "cluster_name=${CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push appointments image
        uses: docker/build-push-action@v6
        with:
          context: ./app
          file: ./app/Dockerfile
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:${{ github.sha }}
            ${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:latest
          cache-from: type=gha,scope=appointments-local
          cache-to: type=gha,mode=max,scope=appointments-local

      - name: Build and push auth image
        uses: docker/build-push-action@v6
        with:
          context: ./auth-service
          file: ./auth-service/Dockerfile
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:${{ github.sha }}
            ${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:latest
          cache-from: type=gha,scope=auth-local
          cache-to: type=gha,mode=max,scope=auth-local

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ steps.cluster.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      - name: Setup Helm
        uses: azure/setup-helm@v4

      - name: Install eksctl
        run: |
          set -euo pipefail
          if command -v eksctl >/dev/null 2>&1; then
            echo "eksctl already installed"
            exit 0
          fi
          curl -fsSL --retry 3 --retry-delay 2 \
            "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \
            | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/eksctl
          eksctl version

      - name: Ensure AWS Load Balancer Controller
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          if kubectl get deployment aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
            echo "aws-load-balancer-controller already present."
          else
            echo "aws-load-balancer-controller missing; installing..."
            VPC_ID="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --query 'cluster.resourcesVpcConfig.vpcId' --output text)"
            if [ -z "${VPC_ID}" ] || [ "${VPC_ID}" = "None" ]; then
              echo "Unable to resolve VPC ID for cluster ${CLUSTER_NAME}"
              exit 1
            fi
            chmod +x scripts/aws/install-alb-controller.sh
            ./scripts/aws/install-alb-controller.sh "${CLUSTER_NAME}" "${AWS_REGION}" "${VPC_ID}"
          fi
          kubectl rollout status deployment/aws-load-balancer-controller -n kube-system --timeout=300s
          kubectl get ingressclass alb >/dev/null 2>&1 || echo "Warning: ingressclass 'alb' not found; relying on legacy annotation."

      - name: Ensure namespace exists
        run: |
          kubectl get namespace ${{ steps.vars.outputs.namespace }} >/dev/null 2>&1 || kubectl create namespace ${{ steps.vars.outputs.namespace }}

      - name: Ensure test nodegroup capacity
        if: steps.vars.outputs.env_name == 'test'
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          NODEGROUP_NAME="cloud-projet-${{ steps.vars.outputs.env_name }}-ng"
          TARGET_NODES=1

          MIN_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.minSize' --output text)"
          MAX_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.maxSize' --output text)"
          DESIRED_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.desiredSize' --output text)"

          if [ "${MAX_SIZE}" -lt "${TARGET_NODES}" ]; then
            echo "Nodegroup ${NODEGROUP_NAME} maxSize=${MAX_SIZE} is too low. Set maxSize>=${TARGET_NODES} in Terraform."
            exit 1
          fi

          if [ "${DESIRED_SIZE}" -ne "${TARGET_NODES}" ] || [ "${MIN_SIZE}" -ne "${TARGET_NODES}" ]; then
            aws eks update-nodegroup-config \
              --cluster-name "${CLUSTER_NAME}" \
              --nodegroup-name "${NODEGROUP_NAME}" \
              --region "${AWS_REGION}" \
              --scaling-config minSize="${TARGET_NODES}",maxSize="${MAX_SIZE}",desiredSize="${TARGET_NODES}"
            aws eks wait nodegroup-active \
              --cluster-name "${CLUSTER_NAME}" \
              --nodegroup-name "${NODEGROUP_NAME}" \
              --region "${AWS_REGION}"
          fi

          for i in $(seq 1 30); do
            READY_NODES="$(kubectl get nodes --no-headers 2>/dev/null | grep -c ' Ready ')"
            if [ "${READY_NODES}" -ge "${TARGET_NODES}" ]; then
              break
            fi
            sleep 10
          done

          if [ "${READY_NODES}" -lt "${TARGET_NODES}" ]; then
            echo "Timed out waiting for at least ${TARGET_NODES} Ready nodes. Current Ready nodes: ${READY_NODES}"
            kubectl get nodes -o wide || true
            exit 1
          fi

      - name: Render manifests
        run: |
          set -euo pipefail
          escape_for_sed() {
            printf '%s' "$1" | sed -e 's/[\/&|]/\\&/g'
          }

          DB_PASSWORD_ESCAPED="$(escape_for_sed "${{ steps.vars.outputs.db_password }}")"
          AUTH_JWT_SECRET_ESCAPED="$(escape_for_sed "${{ steps.vars.outputs.auth_jwt_secret }}")"

          rm -rf rendered
          cp -R k8s/eks rendered
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s/namespace: demo/namespace: ${{ steps.vars.outputs.namespace }}/g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__APPOINTMENTS_IMAGE__|${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:${{ github.sha }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__AUTH_IMAGE__|${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:${{ github.sha }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__SET_IN_CLUSTER__|${DB_PASSWORD_ESCAPED}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__AUTH_JWT_SECRET__|${AUTH_JWT_SECRET_ESCAPED}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__APPOINTMENTS_BASE_URL__|http://placeholder.local|g"
          if [ "${{ steps.vars.outputs.env_name }}" = "test" ]; then
            sed -i 's/replicas: 3/replicas: 1/g' rendered/07-api-deployment.yaml
            sed -i 's/replicas: 3/replicas: 1/g' rendered/11-auth-deployment.yaml
            sed -i 's/minReplicas: 2/minReplicas: 1/g' rendered/14-api-hpa.yaml
            sed -i 's/minReplicas: 2/minReplicas: 1/g' rendered/15-auth-hpa.yaml
          fi

      - name: Apply and wait for postgres first
        run: |
          set -euo pipefail
          NS="${{ steps.vars.outputs.namespace }}"

          show_debug() {
            echo "=== Postgres deployment (${NS}) ==="
            kubectl get deployment/postgres -n "${NS}" -o wide || true
            kubectl describe deployment/postgres -n "${NS}" || true
            echo "=== Postgres PVC (${NS}) ==="
            kubectl get pvc/postgres-data -n "${NS}" -o wide || true
            kubectl describe pvc/postgres-data -n "${NS}" || true
            echo "=== Postgres pods (${NS}) ==="
            kubectl get pods -n "${NS}" -l app=postgres -o wide || true
            kubectl describe pods -n "${NS}" -l app=postgres || true
            echo "=== Postgres logs (${NS}) ==="
            kubectl logs -n "${NS}" -l app=postgres --all-containers --tail=200 || true
            echo "=== Recent events (${NS}) ==="
            kubectl get events -n "${NS}" --sort-by=.metadata.creationTimestamp | tail -n 80 || true
          }

          if ! kubectl get storageclass gp3 >/dev/null 2>&1; then
            echo "StorageClass gp3 not found; creating cloud-projet-gp3 for EBS CSI."
            cat <<'EOF' | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: cloud-projet-gp3
          provisioner: ebs.csi.aws.com
          volumeBindingMode: WaitForFirstConsumer
          parameters:
            type: gp3
            fsType: ext4
          allowVolumeExpansion: true
          EOF
            SC_NAME="cloud-projet-gp3"
          else
            SC_NAME="gp3"
          fi

          sed -i "s/storageClassName: gp3/storageClassName: ${SC_NAME}/" rendered/02-postgres-pvc.yaml
          kubectl apply -f rendered/01-postgres-secret.yaml
          kubectl apply -f rendered/02-postgres-pvc.yaml
          kubectl apply -f rendered/03-postgres-deployment.yaml
          kubectl apply -f rendered/04-postgres-service.yaml
          kubectl wait --for=jsonpath='{.status.phase}'=Bound pvc/postgres-data -n "${NS}" --timeout=300s || { show_debug; exit 1; }
          kubectl rollout status deployment/postgres -n "${NS}" --timeout=900s || { show_debug; exit 1; }
          kubectl wait --for=condition=Ready pod -l app=postgres -n "${NS}" --timeout=300s || { show_debug; exit 1; }

      - name: Apply remaining manifests
        run: kubectl apply -k rendered

      - name: Wait ingress and patch auth base URL
        run: |
          set -euo pipefail
          NS="${{ steps.vars.outputs.namespace }}"

          show_debug() {
            echo "=== Cluster nodes ==="
            kubectl get nodes -o wide || true
            echo "=== Namespace resources (${NS}) ==="
            kubectl get all -n "${NS}" -o wide || true
            echo "=== Ingress describe (${NS}) ==="
            kubectl describe ingress app-ingress -n "${NS}" || true
            echo "=== Recent namespace events (${NS}) ==="
            kubectl get events -n "${NS}" --sort-by=.metadata.creationTimestamp | tail -n 80 || true
            echo "=== AWS LB controller status/logs ==="
            kubectl get deployment aws-load-balancer-controller -n kube-system -o wide || true
            kubectl logs -n kube-system deployment/aws-load-balancer-controller --tail=200 || true
            echo "=== Appointments describe ==="
            kubectl describe deployment/appointments-api -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=appointments-api || true
            echo "=== Appointments logs ==="
            kubectl logs -n "${NS}" -l app=appointments-api --all-containers --tail=200 || true
            echo "=== Auth describe ==="
            kubectl describe deployment/auth-api -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=auth-api || true
            echo "=== Auth logs ==="
            kubectl logs -n "${NS}" -l app=auth-api --all-containers --tail=200 || true
            echo "=== Postgres describe/logs ==="
            kubectl describe deployment/postgres -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=postgres || true
            kubectl logs -n "${NS}" -l app=postgres --all-containers --tail=200 || true
          }

          kubectl rollout status deployment/postgres -n "${NS}" --timeout=900s || { show_debug; exit 1; }
          kubectl rollout status deployment/appointments-api -n "${NS}" --timeout=600s || { show_debug; exit 1; }
          kubectl rollout status deployment/auth-api -n "${NS}" --timeout=600s || { show_debug; exit 1; }
          for i in $(seq 1 60); do
            ALB_HOST=$(kubectl get ingress app-ingress -n "${NS}" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || true)
            if [ -n "$ALB_HOST" ]; then break; fi
            sleep 10
          done
          if [ -z "$ALB_HOST" ]; then
            echo "ALB hostname not ready yet"
            show_debug
            exit 1
          fi
          kubectl patch configmap auth-config -n "${NS}" --type merge -p "{\"data\":{\"AUTH_APP_BASE_URL\":\"http://$ALB_HOST\"}}"
          kubectl rollout restart deployment/auth-api -n "${NS}"
          echo "Application URL: http://$ALB_HOST"
