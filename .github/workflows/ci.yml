name: CI-CD

on:
  push:
    branches: ["main", "test"]
  pull_request:
  workflow_dispatch:

permissions:
  contents: read
  security-events: write
  id-token: write

env:
  AWS_REGION: eu-west-3

jobs:
  build_test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_DB: notesdb
          POSTGRES_USER: notes
          POSTGRES_PASSWORD: notes_pwd
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U notes -d notesdb"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
    steps:
      - uses: actions/checkout@v4

      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "21"

      - name: Build and test appointments service
        env:
          DB_URL: jdbc:postgresql://localhost:5432/notesdb
          DB_USER: notes
          DB_PASSWORD: notes_pwd
        run: mvn -f app/pom.xml -q test package

      - name: Build and test auth service
        env:
          DB_URL: jdbc:postgresql://localhost:5432/notesdb
          DB_USER: notes
          DB_PASSWORD: notes_pwd
        run: mvn -f auth-service/pom.xml -q test package

  codeql_scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "21"
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: java,javascript
      - name: Build for CodeQL
        run: |
          mvn -f app/pom.xml -q -DskipTests package
          mvn -f auth-service/pom.xml -q -DskipTests package
      - name: Analyze with CodeQL
        uses: github/codeql-action/analyze@v3

  secret_scanning:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Scan secrets with Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  image_security:
    needs: [build_test, codeql_scan, secret_scanning]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build appointments image locally
        uses: docker/build-push-action@v6
        with:
          context: ./app
          file: ./app/Dockerfile
          push: false
          load: true
          tags: appointments-local:${{ github.sha }}
      - name: Build auth image locally
        uses: docker/build-push-action@v6
        with:
          context: ./auth-service
          file: ./auth-service/Dockerfile
          push: false
          load: true
          tags: auth-local:${{ github.sha }}
      - name: Scan appointments image with Trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: appointments-local:${{ github.sha }}
          format: table
          vuln-type: os,library
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: "1"
      - name: Scan auth image with Trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: auth-local:${{ github.sha }}
          format: table
          vuln-type: os,library
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: "1"

  deploy_precheck:
    needs: image_security
    runs-on: ubuntu-latest
    outputs:
      deploy_enabled: ${{ steps.precheck.outputs.deploy_enabled }}
      deploy_reason: ${{ steps.precheck.outputs.deploy_reason }}
    steps:
      - name: Evaluate deploy prerequisites
        id: precheck
        env:
          AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}
          TF_LOCK_TABLE: ${{ secrets.TF_LOCK_TABLE }}
          EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
          DB_PASSWORD_MAIN: ${{ secrets.DB_PASSWORD_MAIN }}
          DB_PASSWORD_TEST: ${{ secrets.DB_PASSWORD_TEST }}
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          ALLOWED_EVENT=false
          if [ "${GITHUB_EVENT_NAME}" = "push" ] && { [ "$BRANCH" = "main" ] || [ "$BRANCH" = "test" ]; }; then
            ALLOWED_EVENT=true
          elif [ "${GITHUB_EVENT_NAME}" = "pull_request" ] && { [ "$BRANCH" = "main" ] || [ "$BRANCH" = "test" ]; } && [ "${{ github.event.pull_request.head.repo.fork }}" = "false" ]; then
            ALLOWED_EVENT=true
          fi

          REQUIRED_VARS="AWS_ROLE_TO_ASSUME TF_STATE_BUCKET TF_LOCK_TABLE EKS_CLUSTER_NAME"
          if [ "$BRANCH" = "main" ]; then
            REQUIRED_VARS="$REQUIRED_VARS DB_PASSWORD_MAIN"
          elif [ "$BRANCH" = "test" ]; then
            REQUIRED_VARS="$REQUIRED_VARS DB_PASSWORD_TEST"
          fi

          MISSING=""
          for v in $REQUIRED_VARS; do
            if [ -z "${!v}" ]; then
              MISSING="$MISSING $v"
            fi
          done

          INVALID_ARN=false
          if [ -n "${AWS_ROLE_TO_ASSUME}" ]; then
            if ! [[ "${AWS_ROLE_TO_ASSUME}" =~ ^arn:aws:iam::[0-9]{12}:role/.+ ]]; then
              INVALID_ARN=true
            fi
          fi

          INVALID_CLUSTER_NAME=false
          if [ -n "${EKS_CLUSTER_NAME}" ]; then
            EKS_CLUSTER_NAME_TRIMMED="$(echo "${EKS_CLUSTER_NAME}" | xargs)"
            if ! [[ "${EKS_CLUSTER_NAME_TRIMMED}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
              INVALID_CLUSTER_NAME=true
            fi
          fi

          if [ "$ALLOWED_EVENT" = "true" ] && [ -z "$MISSING" ] && [ "$INVALID_ARN" = "false" ] && [ "$INVALID_CLUSTER_NAME" = "false" ]; then
            echo "deploy_enabled=true" >> "$GITHUB_OUTPUT"
            echo "deploy_reason=ok" >> "$GITHUB_OUTPUT"
            echo "Deploy precheck passed."
          else
            echo "deploy_enabled=false" >> "$GITHUB_OUTPUT"
            REASON="precheck_blocked"
            echo "Deploy precheck failed or not applicable."
            if [ "$ALLOWED_EVENT" != "true" ]; then
              echo "Reason: event/branch not eligible for deploy."
              REASON="event_or_branch_not_eligible"
            fi
            if [ -n "$MISSING" ]; then
              echo "Reason: missing required secrets:$MISSING"
              REASON="missing_required_secrets:$MISSING"
            fi
            if [ "$INVALID_ARN" = "true" ]; then
              echo "Reason: AWS_ROLE_TO_ASSUME is not a valid IAM role ARN."
              REASON="invalid_aws_role_arn"
            fi
            if [ "$INVALID_CLUSTER_NAME" = "true" ]; then
              echo "Reason: EKS_CLUSTER_NAME is invalid (allowed: letters, numbers, '_' and '-', no spaces)."
              REASON="invalid_eks_cluster_name"
            fi
            echo "deploy_reason=$REASON" >> "$GITHUB_OUTPUT"
            if [ "$ALLOWED_EVENT" = "true" ] && { [ -n "$MISSING" ] || [ "$INVALID_ARN" = "true" ] || [ "$INVALID_CLUSTER_NAME" = "true" ]; }; then
              echo "Failing precheck because deploy is eligible but prerequisites are invalid."
              exit 1
            fi
          fi

  terraform_infra:
    if: needs.deploy_precheck.outputs.deploy_enabled == 'true'
    needs: [image_security, deploy_precheck]
    runs-on: ubuntu-latest
    concurrency:
      group: terraform-eks-shared
      cancel-in-progress: false
    outputs:
      cluster_name: ${{ steps.tfout.outputs.cluster_name }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Normalize backend config
        id: backend
        env:
          TF_STATE_BUCKET_RAW: ${{ secrets.TF_STATE_BUCKET }}
          TF_LOCK_TABLE_RAW: ${{ secrets.TF_LOCK_TABLE }}
        run: |
          set -euo pipefail
          TF_STATE_BUCKET="$(echo "${TF_STATE_BUCKET_RAW}" | xargs)"
          TF_LOCK_TABLE="$(echo "${TF_LOCK_TABLE_RAW}" | xargs)"

          if [ -z "${TF_STATE_BUCKET}" ] || [ -z "${TF_LOCK_TABLE}" ]; then
            echo "TF_STATE_BUCKET and TF_LOCK_TABLE must be non-empty."
            exit 1
          fi

          if ! [[ "${TF_STATE_BUCKET}" =~ ^[a-z0-9][a-z0-9.-]{1,61}[a-z0-9]$ ]]; then
            echo "Invalid TF_STATE_BUCKET format: must be a valid S3 bucket name."
            exit 1
          fi

          if ! [[ "${TF_LOCK_TABLE}" =~ ^[A-Za-z0-9_.-]{3,255}$ ]]; then
            echo "Invalid TF_LOCK_TABLE format: must match DynamoDB table naming rules."
            exit 1
          fi

          echo "tf_state_bucket=${TF_STATE_BUCKET}" >> "$GITHUB_OUTPUT"
          echo "tf_lock_table=${TF_LOCK_TABLE}" >> "$GITHUB_OUTPUT"

      - name: Ensure Terraform backend exists
        env:
          TF_STATE_BUCKET: ${{ steps.backend.outputs.tf_state_bucket }}
          TF_LOCK_TABLE: ${{ steps.backend.outputs.tf_lock_table }}
        run: |
          set -euo pipefail

          if ! aws s3api head-bucket --bucket "${TF_STATE_BUCKET}" 2>/dev/null; then
            echo "Creating missing S3 backend bucket: ${TF_STATE_BUCKET}"
            aws s3api create-bucket \
              --bucket "${TF_STATE_BUCKET}" \
              --region "${AWS_REGION}" \
              --create-bucket-configuration LocationConstraint="${AWS_REGION}"
            aws s3api wait bucket-exists --bucket "${TF_STATE_BUCKET}"
          else
            echo "S3 backend bucket exists: ${TF_STATE_BUCKET}"
          fi

          aws s3api head-bucket --bucket "${TF_STATE_BUCKET}" >/dev/null

          aws s3api put-bucket-versioning \
            --bucket "${TF_STATE_BUCKET}" \
            --versioning-configuration Status=Enabled

          aws s3api put-bucket-encryption \
            --bucket "${TF_STATE_BUCKET}" \
            --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'

          aws s3api put-public-access-block \
            --bucket "${TF_STATE_BUCKET}" \
            --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true

          if ! aws dynamodb describe-table --table-name "${TF_LOCK_TABLE}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            echo "Creating missing DynamoDB lock table: ${TF_LOCK_TABLE}"
            aws dynamodb create-table \
              --table-name "${TF_LOCK_TABLE}" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region "${AWS_REGION}"
            aws dynamodb wait table-exists \
              --table-name "${TF_LOCK_TABLE}" \
              --region "${AWS_REGION}"
          else
            echo "DynamoDB lock table exists: ${TF_LOCK_TABLE}"
          fi

      - name: Resolve env
        id: vars
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          if [ "$BRANCH" = "main" ]; then
            echo "env_name=main" >> $GITHUB_OUTPUT
          else
            echo "env_name=test" >> $GITHUB_OUTPUT
          fi

      - name: Normalize cluster name
        id: cluster
        env:
          EKS_CLUSTER_NAME_RAW: ${{ secrets.EKS_CLUSTER_NAME }}
        run: |
          set -euo pipefail
          EKS_CLUSTER_NAME="$(echo "${EKS_CLUSTER_NAME_RAW}" | xargs)"
          if [ -z "${EKS_CLUSTER_NAME}" ]; then
            echo "EKS_CLUSTER_NAME must be non-empty."
            exit 1
          fi
          if ! [[ "${EKS_CLUSTER_NAME}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
            echo "EKS_CLUSTER_NAME is invalid. Allowed pattern: ^[0-9A-Za-z][0-9A-Za-z_-]*$"
            exit 1
          fi
          echo "cluster_name=${EKS_CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

      - name: Terraform init
        working-directory: infra/terraform/eks
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ steps.backend.outputs.tf_state_bucket }}" \
            -backend-config="key=cloud-projet/eks/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ steps.backend.outputs.tf_lock_table }}" \
            -backend-config="encrypt=true"

      - name: Terraform validate
        working-directory: infra/terraform/eks
        run: terraform validate

      - name: Import existing AWS resources into Terraform state
        working-directory: infra/terraform/eks
        run: |
          set -euo pipefail

          ENV_NAME="${{ steps.vars.outputs.env_name }}"
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          NAME_PREFIX="cloud-projet-${ENV_NAME}"
          CLUSTER_ROLE_NAME="${NAME_PREFIX}-eks-cluster-role"
          NODES_ROLE_NAME="${NAME_PREFIX}-eks-nodes-role"
          APPOINTMENTS_REPO="${NAME_PREFIX}/appointments-api"
          AUTH_REPO="${NAME_PREFIX}/auth-api"
          NODEGROUP_NAME="${NAME_PREFIX}-ng"

          import_if_missing() {
            local addr="$1"
            local id="$2"
            if terraform state show "$addr" >/dev/null 2>&1; then
              echo "State already has ${addr}"
            else
              echo "Importing ${addr} <- ${id}"
              terraform import "$addr" "$id"
            fi
          }

          role_has_policy() {
            local role_name="$1"
            local policy_arn="$2"
            aws iam list-attached-role-policies \
              --role-name "${role_name}" \
              --query "AttachedPolicies[?PolicyArn=='${policy_arn}'].PolicyArn" \
              --output text | grep -Fxq "${policy_arn}"
          }

          if aws iam get-role --role-name "${CLUSTER_ROLE_NAME}" >/dev/null 2>&1; then
            import_if_missing aws_iam_role.eks_cluster "${CLUSTER_ROLE_NAME}"
            if role_has_policy "${CLUSTER_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"; then
              import_if_missing aws_iam_role_policy_attachment.eks_cluster "${CLUSTER_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
            fi
          fi

          if aws iam get-role --role-name "${NODES_ROLE_NAME}" >/dev/null 2>&1; then
            import_if_missing aws_iam_role.eks_nodes "${NODES_ROLE_NAME}"
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"; then
              import_if_missing aws_iam_role_policy_attachment.node_worker_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
            fi
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"; then
              import_if_missing aws_iam_role_policy_attachment.node_cni_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
            fi
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"; then
              import_if_missing aws_iam_role_policy_attachment.node_ecr_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
            fi
          fi

          if aws ecr describe-repositories --repository-names "${APPOINTMENTS_REPO}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_ecr_repository.appointments "${APPOINTMENTS_REPO}"
          fi

          if aws ecr describe-repositories --repository-names "${AUTH_REPO}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_ecr_repository.auth "${AUTH_REPO}"
          fi

          if aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_eks_cluster.this "${CLUSTER_NAME}"
          fi

          if aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_eks_node_group.default "${CLUSTER_NAME}:${NODEGROUP_NAME}"
          fi

      - name: Terraform plan
        working-directory: infra/terraform/eks
        run: |
          terraform plan -out=tfplan \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="environment=${{ steps.vars.outputs.env_name }}" \
            -var="cluster_name=${{ steps.cluster.outputs.cluster_name }}"

      - name: Terraform apply
        working-directory: infra/terraform/eks
        run: terraform apply -auto-approve tfplan

      - name: Capture terraform outputs
        id: tfout
        working-directory: infra/terraform/eks
        run: |
          set -euo pipefail
          CLUSTER_NAME="$(terraform output -raw cluster_name | xargs)"
          if [ -z "${CLUSTER_NAME}" ]; then
            echo "Terraform output 'cluster_name' is empty."
            exit 1
          fi
          echo "cluster_name=${CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

  deploy_aws_eks:
    if: needs.deploy_precheck.outputs.deploy_enabled == 'true'
    needs: [image_security, deploy_precheck, terraform_infra]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Resolve environment
        id: vars
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          if [ "$BRANCH" = "main" ]; then
            echo "namespace=demo-main" >> $GITHUB_OUTPUT
            echo "db_password=${{ secrets.DB_PASSWORD_MAIN }}" >> $GITHUB_OUTPUT
            echo "env_name=main" >> $GITHUB_OUTPUT
          else
            echo "namespace=demo-test" >> $GITHUB_OUTPUT
            echo "db_password=${{ secrets.DB_PASSWORD_TEST }}" >> $GITHUB_OUTPUT
            echo "env_name=test" >> $GITHUB_OUTPUT
          fi

      - name: Resolve cluster name
        id: cluster
        env:
          TF_CLUSTER_NAME_RAW: ${{ needs.terraform_infra.outputs.cluster_name }}
          EKS_CLUSTER_NAME_RAW: ${{ secrets.EKS_CLUSTER_NAME }}
        run: |
          set -euo pipefail
          TF_CLUSTER_NAME="$(echo "${TF_CLUSTER_NAME_RAW}" | xargs)"
          EKS_CLUSTER_NAME="$(echo "${EKS_CLUSTER_NAME_RAW}" | xargs)"

          CLUSTER_NAME="${TF_CLUSTER_NAME}"
          if [ -z "${CLUSTER_NAME}" ]; then
            CLUSTER_NAME="${EKS_CLUSTER_NAME}"
          fi

          if [ -z "${CLUSTER_NAME}" ]; then
            echo "Cluster name is empty from both Terraform output and EKS_CLUSTER_NAME secret."
            exit 1
          fi

          if ! [[ "${CLUSTER_NAME}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
            echo "Cluster name is invalid. Allowed pattern: ^[0-9A-Za-z][0-9A-Za-z_-]*$"
            exit 1
          fi

          echo "cluster_name=${CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push appointments image
        uses: docker/build-push-action@v6
        with:
          context: ./app
          file: ./app/Dockerfile
          push: true
          tags: ${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:${{ github.sha }},${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:latest

      - name: Build and push auth image
        uses: docker/build-push-action@v6
        with:
          context: ./auth-service
          file: ./auth-service/Dockerfile
          push: true
          tags: ${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:${{ github.sha }},${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:latest

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ steps.cluster.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      - name: Ensure namespace exists
        run: |
          kubectl get namespace ${{ steps.vars.outputs.namespace }} >/dev/null 2>&1 || kubectl create namespace ${{ steps.vars.outputs.namespace }}

      - name: Ensure test nodegroup capacity
        if: steps.vars.outputs.env_name == 'test'
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          NODEGROUP_NAME="cloud-projet-${{ steps.vars.outputs.env_name }}-ng"

          MIN_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.minSize' --output text)"
          MAX_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.maxSize' --output text)"
          DESIRED_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.desiredSize' --output text)"

          if [ "${MAX_SIZE}" -lt 2 ]; then
            echo "Nodegroup ${NODEGROUP_NAME} maxSize=${MAX_SIZE} is too low. Set maxSize>=2 in Terraform."
            exit 1
          fi

          TARGET_MIN="${MIN_SIZE}"
          if [ "${TARGET_MIN}" -lt 2 ]; then
            TARGET_MIN=2
          fi

          if [ "${DESIRED_SIZE}" -lt 2 ] || [ "${MIN_SIZE}" -lt 2 ]; then
            aws eks update-nodegroup-config \
              --cluster-name "${CLUSTER_NAME}" \
              --nodegroup-name "${NODEGROUP_NAME}" \
              --region "${AWS_REGION}" \
              --scaling-config minSize="${TARGET_MIN}",maxSize="${MAX_SIZE}",desiredSize=2
            aws eks wait nodegroup-active \
              --cluster-name "${CLUSTER_NAME}" \
              --nodegroup-name "${NODEGROUP_NAME}" \
              --region "${AWS_REGION}"
          fi

          for i in $(seq 1 30); do
            READY_NODES="$(kubectl get nodes --no-headers 2>/dev/null | awk '$2=="Ready" {c++} END {print c+0}')"
            if [ "${READY_NODES}" -ge 2 ]; then
              break
            fi
            sleep 10
          done

          if [ "${READY_NODES}" -lt 2 ]; then
            echo "Timed out waiting for at least 2 Ready nodes. Current Ready nodes: ${READY_NODES}"
            kubectl get nodes -o wide || true
            exit 1
          fi

      - name: Resolve storage class
        id: storage
        run: |
          set -euo pipefail
          DEFAULT_SC="$(kubectl get storageclass -o jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.metadata.annotations.storageclass\.kubernetes\.io/is-default-class}{'\t'}{.metadata.annotations.storageclass\.beta\.kubernetes\.io/is-default-class}{'\n'}{end}" | awk '$2=="true" || $3=="true" {print $1; exit}')"
          if [ -z "${DEFAULT_SC}" ]; then
            DEFAULT_SC="$(kubectl get storageclass -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          fi
          if [ -z "${DEFAULT_SC}" ]; then
            echo "No Kubernetes StorageClass found in cluster."
            exit 1
          fi
          echo "storage_class=${DEFAULT_SC}" >> "$GITHUB_OUTPUT"
          echo "Using storage class: ${DEFAULT_SC}"

      - name: Render manifests
        run: |
          set -euo pipefail
          rm -rf rendered
          cp -R k8s/eks rendered
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s/namespace: demo/namespace: ${{ steps.vars.outputs.namespace }}/g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__APPOINTMENTS_IMAGE__|${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:${{ github.sha }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__AUTH_IMAGE__|${{ steps.login-ecr.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:${{ github.sha }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__SET_IN_CLUSTER__|${{ steps.vars.outputs.db_password }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__APPOINTMENTS_BASE_URL__|http://placeholder.local|g"
          sed -i "s/^  storageClassName: .*/  storageClassName: ${{ steps.storage.outputs.storage_class }}/" rendered/02-postgres-pvc.yaml
          if [ "${{ steps.vars.outputs.env_name }}" = "test" ]; then
            sed -i 's/replicas: 3/replicas: 1/g' rendered/07-api-deployment.yaml
            sed -i 's/replicas: 3/replicas: 1/g' rendered/11-auth-deployment.yaml
          fi

      - name: Cleanup pending postgres pvc in test
        if: steps.vars.outputs.env_name == 'test'
        run: |
          set -euo pipefail
          NS="${{ steps.vars.outputs.namespace }}"
          if kubectl get pvc postgres-pvc -n "${NS}" >/dev/null 2>&1; then
            PVC_PHASE="$(kubectl get pvc postgres-pvc -n "${NS}" -o jsonpath='{.status.phase}')"
            if [ "${PVC_PHASE}" != "Bound" ]; then
              kubectl delete deployment postgres -n "${NS}" --ignore-not-found=true
              kubectl delete pvc postgres-pvc -n "${NS}" --ignore-not-found=true
            fi
          fi

      - name: Apply manifests
        run: kubectl apply -k rendered

      - name: Wait ingress and patch auth base URL
        run: |
          set -euo pipefail
          NS="${{ steps.vars.outputs.namespace }}"

          show_debug() {
            echo "=== Cluster nodes ==="
            kubectl get nodes -o wide || true
            echo "=== Namespace resources (${NS}) ==="
            kubectl get all -n "${NS}" -o wide || true
            echo "=== Recent namespace events (${NS}) ==="
            kubectl get events -n "${NS}" --sort-by=.metadata.creationTimestamp | tail -n 80 || true
            echo "=== Appointments describe ==="
            kubectl describe deployment/appointments-api -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=appointments-api || true
            echo "=== Appointments logs ==="
            kubectl logs -n "${NS}" -l app=appointments-api --all-containers --tail=200 || true
            echo "=== Auth describe ==="
            kubectl describe deployment/auth-api -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=auth-api || true
            echo "=== Auth logs ==="
            kubectl logs -n "${NS}" -l app=auth-api --all-containers --tail=200 || true
            echo "=== Postgres describe/logs ==="
            kubectl describe deployment/postgres -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=postgres || true
            kubectl logs -n "${NS}" -l app=postgres --all-containers --tail=200 || true
          }

          kubectl rollout status deployment/postgres -n "${NS}" --timeout=300s || { show_debug; exit 1; }
          kubectl rollout status deployment/appointments-api -n "${NS}" --timeout=600s || { show_debug; exit 1; }
          kubectl rollout status deployment/auth-api -n "${NS}" --timeout=600s || { show_debug; exit 1; }
          for i in $(seq 1 30); do
            ALB_HOST=$(kubectl get ingress app-ingress -n "${NS}" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || true)
            if [ -n "$ALB_HOST" ]; then break; fi
            sleep 10
          done
          if [ -z "$ALB_HOST" ]; then
            echo "ALB hostname not ready yet"
            show_debug
            exit 1
          fi
          kubectl patch configmap auth-config -n "${NS}" --type merge -p "{\"data\":{\"AUTH_APP_BASE_URL\":\"http://$ALB_HOST\"}}"
          kubectl rollout restart deployment/auth-api -n "${NS}"
          echo "Application URL: http://$ALB_HOST"
