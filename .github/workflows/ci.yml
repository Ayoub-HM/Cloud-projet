name: CI-CD

on:
  push:
    branches: ["main", "test"]
  pull_request:
  workflow_dispatch:

permissions:
  contents: read
  security-events: write
  id-token: write

env:
  AWS_REGION: eu-west-3

concurrency:
  group: ci-cd-${{ github.ref }}
  cancel-in-progress: true

jobs:
  changes:
    runs-on: ubuntu-latest
    outputs:
      app: ${{ steps.filter.outputs.app }}
      auth: ${{ steps.filter.outputs.auth }}
      k8s: ${{ steps.filter.outputs.k8s }}
      infra: ${{ steps.filter.outputs.infra }}
      workflows: ${{ steps.filter.outputs.workflows }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Detect changed areas
        id: filter
        uses: dorny/paths-filter@v3
        with:
          filters: |
            app:
              - 'app/**'
            auth:
              - 'auth-service/**'
            k8s:
              - 'k8s/**'
            infra:
              - 'infra/terraform/eks/**'
            workflows:
              - '.github/workflows/**'

  build_test:
    needs: changes
    if: ${{ github.event_name == 'workflow_dispatch' || needs.changes.outputs.app == 'true' || needs.changes.outputs.auth == 'true' || needs.changes.outputs.workflows == 'true' }}
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_DB: notesdb
          POSTGRES_USER: notes
          POSTGRES_PASSWORD: notes_pwd
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U notes -d notesdb"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
    steps:
      - uses: actions/checkout@v4

      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "21"
          cache: maven

      - name: Build and test appointments service
        env:
          DB_URL: jdbc:postgresql://localhost:5432/notesdb
          DB_USER: notes
          DB_PASSWORD: notes_pwd
        run: mvn -f app/pom.xml -q test

      - name: Build and test auth service
        env:
          DB_URL: jdbc:postgresql://localhost:5432/notesdb
          DB_USER: notes
          DB_PASSWORD: notes_pwd
        run: mvn -f auth-service/pom.xml -q test

  codeql_scan:
    needs: changes
    if: ${{ github.event_name == 'workflow_dispatch' || needs.changes.outputs.app == 'true' || needs.changes.outputs.auth == 'true' || needs.changes.outputs.workflows == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "21"
          cache: maven
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: java,javascript
      - name: Build for CodeQL
        run: |
          mvn -f app/pom.xml -q -DskipTests package
          mvn -f auth-service/pom.xml -q -DskipTests package
      - name: Analyze with CodeQL
        uses: github/codeql-action/analyze@v3

  secret_scanning:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Scan secrets with Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  image_security:
    needs: [changes, build_test, codeql_scan, secret_scanning]
    if: ${{ github.event_name == 'workflow_dispatch' || needs.changes.outputs.app == 'true' || needs.changes.outputs.auth == 'true' || needs.changes.outputs.workflows == 'true' }}
    runs-on: ubuntu-latest
    env:
      TRIVY_CACHE_DIR: ${{ github.workspace }}/.trivycache
    steps:
      - uses: actions/checkout@v4

      - name: Cache Trivy DB
        uses: actions/cache@v4
        with:
          path: ${{ env.TRIVY_CACHE_DIR }}
          key: trivy-db-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            trivy-db-${{ runner.os }}-

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build appointments image locally
        uses: docker/build-push-action@v6
        with:
          context: ./app
          file: ./app/Dockerfile
          push: false
          load: true
          tags: appointments-local:${{ github.sha }}
          cache-from: type=gha,scope=appointments-local
          cache-to: type=gha,mode=max,scope=appointments-local
      - name: Build auth image locally
        uses: docker/build-push-action@v6
        with:
          context: ./auth-service
          file: ./auth-service/Dockerfile
          push: false
          load: true
          tags: auth-local:${{ github.sha }}
          cache-from: type=gha,scope=auth-local
          cache-to: type=gha,mode=max,scope=auth-local
      - name: Scan appointments image with Trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: appointments-local:${{ github.sha }}
          format: table
          vuln-type: os,library
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: "1"
      - name: Scan auth image with Trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: auth-local:${{ github.sha }}
          format: table
          vuln-type: os,library
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: "1"

  deploy_precheck:
    runs-on: ubuntu-latest
    outputs:
      deploy_enabled: ${{ steps.precheck.outputs.deploy_enabled }}
      deploy_reason: ${{ steps.precheck.outputs.deploy_reason }}
    steps:
      - name: Evaluate deploy prerequisites
        id: precheck
        env:
          AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}
          TF_LOCK_TABLE: ${{ secrets.TF_LOCK_TABLE }}
          EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
          DB_PASSWORD_MAIN: ${{ secrets.DB_PASSWORD_MAIN }}
          DB_PASSWORD_TEST: ${{ secrets.DB_PASSWORD_TEST }}
          AUTH_JWT_SECRET_MAIN: ${{ secrets.AUTH_JWT_SECRET_MAIN }}
          AUTH_JWT_SECRET_TEST: ${{ secrets.AUTH_JWT_SECRET_TEST }}
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          ALLOWED_EVENT=false
          if [ "${GITHUB_EVENT_NAME}" = "push" ] && { [ "$BRANCH" = "main" ] || [ "$BRANCH" = "test" ]; }; then
            ALLOWED_EVENT=true
          elif [ "${GITHUB_EVENT_NAME}" = "pull_request" ] && { [ "$BRANCH" = "main" ] || [ "$BRANCH" = "test" ]; } && [ "${{ github.event.pull_request.head.repo.fork }}" = "false" ]; then
            ALLOWED_EVENT=true
          fi

          REQUIRED_VARS="AWS_ROLE_TO_ASSUME TF_STATE_BUCKET TF_LOCK_TABLE EKS_CLUSTER_NAME"
          if [ "$BRANCH" = "main" ]; then
            REQUIRED_VARS="$REQUIRED_VARS DB_PASSWORD_MAIN"
          elif [ "$BRANCH" = "test" ]; then
            REQUIRED_VARS="$REQUIRED_VARS DB_PASSWORD_TEST"
          fi

          MISSING=""
          for v in $REQUIRED_VARS; do
            if [ -z "${!v}" ]; then
              MISSING="$MISSING $v"
            fi
          done

          INVALID_ARN=false
          if [ -n "${AWS_ROLE_TO_ASSUME}" ]; then
            if ! [[ "${AWS_ROLE_TO_ASSUME}" =~ ^arn:aws:iam::[0-9]{12}:role/.+ ]]; then
              INVALID_ARN=true
            fi
          fi

          INVALID_CLUSTER_NAME=false
          if [ -n "${EKS_CLUSTER_NAME}" ]; then
            EKS_CLUSTER_NAME_TRIMMED="$(echo "${EKS_CLUSTER_NAME}" | xargs)"
            if ! [[ "${EKS_CLUSTER_NAME_TRIMMED}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
              INVALID_CLUSTER_NAME=true
            fi
          fi

          JWT_SECRET_TO_CHECK=""
          if [ "$BRANCH" = "main" ]; then
            JWT_SECRET_TO_CHECK="${AUTH_JWT_SECRET_MAIN}"
          elif [ "$BRANCH" = "test" ]; then
            JWT_SECRET_TO_CHECK="${AUTH_JWT_SECRET_TEST}"
          fi

          INVALID_AUTH_JWT_SECRET=false
          if [ -n "${JWT_SECRET_TO_CHECK}" ] && [ "${#JWT_SECRET_TO_CHECK}" -lt 32 ]; then
            INVALID_AUTH_JWT_SECRET=true
          fi

          if [ "$ALLOWED_EVENT" = "true" ] && [ -z "$MISSING" ] && [ "$INVALID_ARN" = "false" ] && [ "$INVALID_CLUSTER_NAME" = "false" ] && [ "$INVALID_AUTH_JWT_SECRET" = "false" ]; then
            echo "deploy_enabled=true" >> "$GITHUB_OUTPUT"
            echo "deploy_reason=ok" >> "$GITHUB_OUTPUT"
            echo "Deploy precheck passed."
          else
            echo "deploy_enabled=false" >> "$GITHUB_OUTPUT"
            REASON="precheck_blocked"
            echo "Deploy precheck failed or not applicable."
            if [ "$ALLOWED_EVENT" != "true" ]; then
              echo "Reason: event/branch not eligible for deploy."
              REASON="event_or_branch_not_eligible"
            fi
            if [ -n "$MISSING" ]; then
              echo "Reason: missing required secrets:$MISSING"
              REASON="missing_required_secrets:$MISSING"
            fi
            if [ "$INVALID_ARN" = "true" ]; then
              echo "Reason: AWS_ROLE_TO_ASSUME is not a valid IAM role ARN."
              REASON="invalid_aws_role_arn"
            fi
            if [ "$INVALID_CLUSTER_NAME" = "true" ]; then
              echo "Reason: EKS_CLUSTER_NAME is invalid (allowed: letters, numbers, '_' and '-', no spaces)."
              REASON="invalid_eks_cluster_name"
            fi
            if [ "$INVALID_AUTH_JWT_SECRET" = "true" ]; then
              echo "Reason: AUTH_JWT_SECRET_<ENV> is too short (minimum 32 characters)."
              REASON="invalid_auth_jwt_secret"
            fi
            echo "deploy_reason=$REASON" >> "$GITHUB_OUTPUT"
            if [ "$ALLOWED_EVENT" = "true" ] && { [ -n "$MISSING" ] || [ "$INVALID_ARN" = "true" ] || [ "$INVALID_CLUSTER_NAME" = "true" ] || [ "$INVALID_AUTH_JWT_SECRET" = "true" ]; }; then
              echo "Failing precheck because deploy is eligible but prerequisites are invalid."
              exit 1
            fi
          fi

  terraform_infra:
    if: ${{ needs.deploy_precheck.outputs.deploy_enabled == 'true' && (github.event_name == 'workflow_dispatch' || needs.changes.outputs.infra == 'true') }}
    needs: [changes, deploy_precheck]
    runs-on: ubuntu-latest
    concurrency:
      group: terraform-eks-shared
      cancel-in-progress: false
    outputs:
      cluster_name: ${{ steps.tfout.outputs.cluster_name }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Normalize backend config
        id: backend
        env:
          TF_STATE_BUCKET_RAW: ${{ secrets.TF_STATE_BUCKET }}
          TF_LOCK_TABLE_RAW: ${{ secrets.TF_LOCK_TABLE }}
        run: |
          set -euo pipefail
          TF_STATE_BUCKET="$(echo "${TF_STATE_BUCKET_RAW}" | xargs)"
          TF_LOCK_TABLE="$(echo "${TF_LOCK_TABLE_RAW}" | xargs)"

          if [ -z "${TF_STATE_BUCKET}" ] || [ -z "${TF_LOCK_TABLE}" ]; then
            echo "TF_STATE_BUCKET and TF_LOCK_TABLE must be non-empty."
            exit 1
          fi

          if ! [[ "${TF_STATE_BUCKET}" =~ ^[a-z0-9][a-z0-9.-]{1,61}[a-z0-9]$ ]]; then
            echo "Invalid TF_STATE_BUCKET format: must be a valid S3 bucket name."
            exit 1
          fi

          if ! [[ "${TF_LOCK_TABLE}" =~ ^[A-Za-z0-9_.-]{3,255}$ ]]; then
            echo "Invalid TF_LOCK_TABLE format: must match DynamoDB table naming rules."
            exit 1
          fi

          echo "tf_state_bucket=${TF_STATE_BUCKET}" >> "$GITHUB_OUTPUT"
          echo "tf_lock_table=${TF_LOCK_TABLE}" >> "$GITHUB_OUTPUT"

      - name: Ensure Terraform backend exists
        env:
          TF_STATE_BUCKET: ${{ steps.backend.outputs.tf_state_bucket }}
          TF_LOCK_TABLE: ${{ steps.backend.outputs.tf_lock_table }}
        run: |
          set -euo pipefail

          if ! aws s3api head-bucket --bucket "${TF_STATE_BUCKET}" 2>/dev/null; then
            echo "Creating missing S3 backend bucket: ${TF_STATE_BUCKET}"
            aws s3api create-bucket \
              --bucket "${TF_STATE_BUCKET}" \
              --region "${AWS_REGION}" \
              --create-bucket-configuration LocationConstraint="${AWS_REGION}"
            aws s3api wait bucket-exists --bucket "${TF_STATE_BUCKET}"
          else
            echo "S3 backend bucket exists: ${TF_STATE_BUCKET}"
          fi

          aws s3api head-bucket --bucket "${TF_STATE_BUCKET}" >/dev/null

          aws s3api put-bucket-versioning \
            --bucket "${TF_STATE_BUCKET}" \
            --versioning-configuration Status=Enabled

          aws s3api put-bucket-encryption \
            --bucket "${TF_STATE_BUCKET}" \
            --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'

          aws s3api put-public-access-block \
            --bucket "${TF_STATE_BUCKET}" \
            --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true

          if ! aws dynamodb describe-table --table-name "${TF_LOCK_TABLE}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            echo "Creating missing DynamoDB lock table: ${TF_LOCK_TABLE}"
            aws dynamodb create-table \
              --table-name "${TF_LOCK_TABLE}" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region "${AWS_REGION}"
            aws dynamodb wait table-exists \
              --table-name "${TF_LOCK_TABLE}" \
              --region "${AWS_REGION}"
          else
            echo "DynamoDB lock table exists: ${TF_LOCK_TABLE}"
          fi

      - name: Resolve env
        id: vars
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          if [ "$BRANCH" = "main" ]; then
            echo "env_name=main" >> $GITHUB_OUTPUT
          else
            echo "env_name=test" >> $GITHUB_OUTPUT
          fi

      - name: Resolve nodegroup sizing
        id: sizing
        run: |
          if [ "${{ steps.vars.outputs.env_name }}" = "main" ]; then
            echo "node_desired_size=2" >> "$GITHUB_OUTPUT"
            echo "node_min_size=2" >> "$GITHUB_OUTPUT"
            echo "node_max_size=4" >> "$GITHUB_OUTPUT"
          else
            echo "node_desired_size=1" >> "$GITHUB_OUTPUT"
            echo "node_min_size=1" >> "$GITHUB_OUTPUT"
            echo "node_max_size=2" >> "$GITHUB_OUTPUT"
          fi

      - name: Normalize cluster name
        id: cluster
        env:
          EKS_CLUSTER_NAME_RAW: ${{ secrets.EKS_CLUSTER_NAME }}
        run: |
          set -euo pipefail
          EKS_CLUSTER_NAME="$(echo "${EKS_CLUSTER_NAME_RAW}" | xargs)"
          if [ -z "${EKS_CLUSTER_NAME}" ]; then
            echo "EKS_CLUSTER_NAME must be non-empty."
            exit 1
          fi
          if ! [[ "${EKS_CLUSTER_NAME}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
            echo "EKS_CLUSTER_NAME is invalid. Allowed pattern: ^[0-9A-Za-z][0-9A-Za-z_-]*$"
            exit 1
          fi
          echo "cluster_name=${EKS_CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

      - name: Terraform init
        working-directory: infra/terraform/eks
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ steps.backend.outputs.tf_state_bucket }}" \
            -backend-config="key=cloud-projet/eks/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="dynamodb_table=${{ steps.backend.outputs.tf_lock_table }}" \
            -backend-config="encrypt=true"

      - name: Terraform validate
        working-directory: infra/terraform/eks
        run: terraform validate

      - name: Import existing AWS resources into Terraform state
        working-directory: infra/terraform/eks
        run: |
          set -euo pipefail

          ENV_NAME="${{ steps.vars.outputs.env_name }}"
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          NAME_PREFIX="cloud-projet-${ENV_NAME}"
          CLUSTER_ROLE_NAME="${NAME_PREFIX}-eks-cluster-role"
          NODES_ROLE_NAME="${NAME_PREFIX}-eks-nodes-role"
          APPOINTMENTS_REPO="${NAME_PREFIX}/appointments-api"
          AUTH_REPO="${NAME_PREFIX}/auth-api"
          NODEGROUP_NAME="${NAME_PREFIX}-ng"

          import_if_missing() {
            local addr="$1"
            local id="$2"
            if terraform state show "$addr" >/dev/null 2>&1; then
              echo "State already has ${addr}"
            else
              echo "Importing ${addr} <- ${id}"
              terraform import "$addr" "$id"
            fi
          }

          role_has_policy() {
            local role_name="$1"
            local policy_arn="$2"
            aws iam list-attached-role-policies \
              --role-name "${role_name}" \
              --query "AttachedPolicies[?PolicyArn=='${policy_arn}'].PolicyArn" \
              --output text | grep -Fxq "${policy_arn}"
          }

          if aws iam get-role --role-name "${CLUSTER_ROLE_NAME}" >/dev/null 2>&1; then
            import_if_missing aws_iam_role.eks_cluster "${CLUSTER_ROLE_NAME}"
            if role_has_policy "${CLUSTER_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"; then
              import_if_missing aws_iam_role_policy_attachment.eks_cluster "${CLUSTER_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
            fi
          fi

          if aws iam get-role --role-name "${NODES_ROLE_NAME}" >/dev/null 2>&1; then
            import_if_missing aws_iam_role.eks_nodes "${NODES_ROLE_NAME}"
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"; then
              import_if_missing aws_iam_role_policy_attachment.node_worker_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
            fi
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"; then
              import_if_missing aws_iam_role_policy_attachment.node_cni_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
            fi
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"; then
              import_if_missing aws_iam_role_policy_attachment.node_ecr_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
            fi
            if role_has_policy "${NODES_ROLE_NAME}" "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"; then
              import_if_missing aws_iam_role_policy_attachment.node_ebs_csi_policy "${NODES_ROLE_NAME}/arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
            fi
          fi

          if aws ecr describe-repositories --repository-names "${APPOINTMENTS_REPO}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_ecr_repository.appointments "${APPOINTMENTS_REPO}"
          fi

          if aws ecr describe-repositories --repository-names "${AUTH_REPO}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_ecr_repository.auth "${AUTH_REPO}"
          fi

          if aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_eks_cluster.this "${CLUSTER_NAME}"
          fi

          if aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            import_if_missing aws_eks_node_group.default "${CLUSTER_NAME}:${NODEGROUP_NAME}"
          fi

      - name: Terraform plan
        working-directory: infra/terraform/eks
        run: |
          terraform plan -out=tfplan \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="environment=${{ steps.vars.outputs.env_name }}" \
            -var="cluster_name=${{ steps.cluster.outputs.cluster_name }}" \
            -var="node_desired_size=${{ steps.sizing.outputs.node_desired_size }}" \
            -var="node_min_size=${{ steps.sizing.outputs.node_min_size }}" \
            -var="node_max_size=${{ steps.sizing.outputs.node_max_size }}"

      - name: Terraform apply
        working-directory: infra/terraform/eks
        run: terraform apply -auto-approve tfplan

      - name: Capture terraform outputs
        id: tfout
        working-directory: infra/terraform/eks
        run: |
          set -euo pipefail
          CLUSTER_NAME="$(terraform output -raw cluster_name | xargs)"
          if [ -z "${CLUSTER_NAME}" ]; then
            echo "Terraform output 'cluster_name' is empty."
            exit 1
          fi
          echo "cluster_name=${CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

  deploy_aws_eks:
    if: ${{ needs.deploy_precheck.outputs.deploy_enabled == 'true' &&
      (github.event_name == 'workflow_dispatch' ||
        needs.changes.outputs.app == 'true' ||
        needs.changes.outputs.auth == 'true' ||
        needs.changes.outputs.k8s == 'true' ||
        needs.changes.outputs.infra == 'true' ||
        needs.changes.outputs.workflows == 'true') &&
      (needs.changes.outputs.infra != 'true' || needs.terraform_infra.result == 'success') &&
      ((needs.changes.outputs.app != 'true' && needs.changes.outputs.auth != 'true') || needs.image_security.result == 'success') }}
    needs: [changes, deploy_precheck, terraform_infra, image_security]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Resolve environment
        id: vars
        run: |
          BRANCH="${GITHUB_BASE_REF:-${GITHUB_REF_NAME}}"
          if [ "$BRANCH" = "main" ]; then
            echo "namespace=demo-main" >> $GITHUB_OUTPUT
            DB_PASSWORD="${{ secrets.DB_PASSWORD_MAIN }}"
            JWT_SECRET="${{ secrets.AUTH_JWT_SECRET_MAIN }}"
            echo "env_name=main" >> $GITHUB_OUTPUT
          else
            echo "namespace=demo-test" >> $GITHUB_OUTPUT
            DB_PASSWORD="${{ secrets.DB_PASSWORD_TEST }}"
            JWT_SECRET="${{ secrets.AUTH_JWT_SECRET_TEST }}"
            echo "env_name=test" >> $GITHUB_OUTPUT
          fi
          if [ -z "${JWT_SECRET}" ]; then
            JWT_SECRET="${DB_PASSWORD}-jwt"
          fi
          if [ "${#JWT_SECRET}" -lt 32 ]; then
            JWT_SECRET="${JWT_SECRET}0123456789abcdef0123456789abcdef"
          fi
          echo "db_password=${DB_PASSWORD}" >> $GITHUB_OUTPUT
          echo "auth_jwt_secret=${JWT_SECRET}" >> $GITHUB_OUTPUT

      - name: Resolve cluster name
        id: cluster
        env:
          TF_CLUSTER_NAME_RAW: ${{ needs.terraform_infra.outputs.cluster_name }}
          EKS_CLUSTER_NAME_RAW: ${{ secrets.EKS_CLUSTER_NAME }}
        run: |
          set -euo pipefail
          TF_CLUSTER_NAME="$(echo "${TF_CLUSTER_NAME_RAW}" | xargs)"
          EKS_CLUSTER_NAME="$(echo "${EKS_CLUSTER_NAME_RAW}" | xargs)"

          CLUSTER_NAME="${TF_CLUSTER_NAME}"
          if [ -z "${CLUSTER_NAME}" ]; then
            CLUSTER_NAME="${EKS_CLUSTER_NAME}"
          fi

          if [ -z "${CLUSTER_NAME}" ]; then
            echo "Cluster name is empty from both Terraform output and EKS_CLUSTER_NAME secret."
            exit 1
          fi

          if ! [[ "${CLUSTER_NAME}" =~ ^[0-9A-Za-z][0-9A-Za-z_-]*$ ]]; then
            echo "Cluster name is invalid. Allowed pattern: ^[0-9A-Za-z][0-9A-Za-z_-]*$"
            exit 1
          fi

          echo "cluster_name=${CLUSTER_NAME}" >> "$GITHUB_OUTPUT"

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ steps.cluster.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      - name: Preflight cluster access
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          STATUS="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --query 'cluster.status' --output text)"
          if [ "${STATUS}" != "ACTIVE" ]; then
            echo "EKS cluster ${CLUSTER_NAME} status is '${STATUS}', expected 'ACTIVE'."
            exit 1
          fi

          kubectl get nodes -o wide
          MISSING_ZONE="$(kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.metadata.labels.topology\\.kubernetes\\.io/zone}{\"\\n\"}{end}' | awk '$2==\"\" {print $1}' || true)"
          if [ -n "${MISSING_ZONE}" ]; then
            echo "Some nodes are missing the label topology.kubernetes.io/zone:"
            echo "${MISSING_ZONE}"
            exit 1
          fi

      - name: Setup Helm
        uses: azure/setup-helm@v4

      - name: Install eksctl
        run: |
          set -euo pipefail
          if command -v eksctl >/dev/null 2>&1; then
            echo "eksctl already installed"
            exit 0
          fi
          curl -fsSL --retry 3 --retry-delay 2 \
            "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \
            | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/eksctl
          eksctl version

      - name: Ensure AWS Load Balancer Controller
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          if kubectl get deployment aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
            echo "aws-load-balancer-controller already present."
          else
            echo "aws-load-balancer-controller missing; installing..."
            VPC_ID="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --query 'cluster.resourcesVpcConfig.vpcId' --output text)"
            if [ -z "${VPC_ID}" ] || [ "${VPC_ID}" = "None" ]; then
              echo "Unable to resolve VPC ID for cluster ${CLUSTER_NAME}"
              exit 1
            fi
            chmod +x scripts/aws/install-alb-controller.sh
            ./scripts/aws/install-alb-controller.sh "${CLUSTER_NAME}" "${AWS_REGION}" "${VPC_ID}"
          fi
          kubectl rollout status deployment/aws-load-balancer-controller -n kube-system --timeout=300s
          kubectl get ingressclass alb >/dev/null 2>&1 || echo "Warning: ingressclass 'alb' not found; relying on legacy annotation."

      - name: Ensure namespace exists
        run: |
          kubectl get namespace ${{ steps.vars.outputs.namespace }} >/dev/null 2>&1 || kubectl create namespace ${{ steps.vars.outputs.namespace }}

      - name: Ensure test nodegroup capacity
        if: steps.vars.outputs.env_name == 'test'
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          NODEGROUP_NAME="cloud-projet-${{ steps.vars.outputs.env_name }}-ng"
          TARGET_NODES=1

          MIN_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.minSize' --output text)"
          MAX_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.maxSize' --output text)"
          DESIRED_SIZE="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.scalingConfig.desiredSize' --output text)"

          if [ "${MAX_SIZE}" -lt "${TARGET_NODES}" ]; then
            echo "Nodegroup ${NODEGROUP_NAME} maxSize=${MAX_SIZE} is too low. Set maxSize>=${TARGET_NODES} in Terraform."
            exit 1
          fi

          if [ "${DESIRED_SIZE}" -ne "${TARGET_NODES}" ] || [ "${MIN_SIZE}" -ne "${TARGET_NODES}" ]; then
            aws eks update-nodegroup-config \
              --cluster-name "${CLUSTER_NAME}" \
              --nodegroup-name "${NODEGROUP_NAME}" \
              --region "${AWS_REGION}" \
              --scaling-config minSize="${TARGET_NODES}",maxSize="${MAX_SIZE}",desiredSize="${TARGET_NODES}"
            aws eks wait nodegroup-active \
              --cluster-name "${CLUSTER_NAME}" \
              --nodegroup-name "${NODEGROUP_NAME}" \
              --region "${AWS_REGION}"
          fi

          for i in $(seq 1 30); do
            READY_NODES="$(kubectl get nodes --no-headers 2>/dev/null | grep -c ' Ready ')"
            if [ "${READY_NODES}" -ge "${TARGET_NODES}" ]; then
              break
            fi
            sleep 10
          done

          if [ "${READY_NODES}" -lt "${TARGET_NODES}" ]; then
            echo "Timed out waiting for at least ${TARGET_NODES} Ready nodes. Current Ready nodes: ${READY_NODES}"
            kubectl get nodes -o wide || true
            exit 1
          fi

      - name: Ensure EBS CSI driver and permissions
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ steps.cluster.outputs.cluster_name }}"
          NODEGROUP_NAME="cloud-projet-${{ steps.vars.outputs.env_name }}-ng"
          EBS_POLICY_ARN="arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
          EBS_CSI_ROLE_NAME="cloud-projet-${{ steps.vars.outputs.env_name }}-eks-ebs-csi-role"

          NODE_ROLE_ARN="$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" --nodegroup-name "${NODEGROUP_NAME}" --region "${AWS_REGION}" --query 'nodegroup.nodeRole' --output text)"
          if [ -z "${NODE_ROLE_ARN}" ] || [ "${NODE_ROLE_ARN}" = "None" ]; then
            echo "Unable to resolve node role for ${NODEGROUP_NAME}"
            exit 1
          fi
          NODE_ROLE_NAME="${NODE_ROLE_ARN##*/}"

          if aws iam list-attached-role-policies --role-name "${NODE_ROLE_NAME}" --query "AttachedPolicies[?PolicyArn=='${EBS_POLICY_ARN}'].PolicyArn" --output text | grep -Fxq "${EBS_POLICY_ARN}"; then
            echo "AmazonEBSCSIDriverPolicy already attached to ${NODE_ROLE_NAME}"
          else
            echo "Attaching AmazonEBSCSIDriverPolicy to ${NODE_ROLE_NAME}"
            aws iam attach-role-policy --role-name "${NODE_ROLE_NAME}" --policy-arn "${EBS_POLICY_ARN}"
          fi

          show_ebs_debug() {
            echo "=== EBS CSI add-on (EKS) ==="
            aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver --region "${AWS_REGION}" || true
            echo "=== EBS CSI deployment (kube-system) ==="
            kubectl -n kube-system get deployment ebs-csi-controller -o wide || true
            kubectl -n kube-system describe deployment ebs-csi-controller || true
            kubectl -n kube-system get daemonset ebs-csi-node -o wide || true
            kubectl -n kube-system describe daemonset ebs-csi-node || true
            kubectl get csinode || true
            echo "=== EBS CSI pods (kube-system) ==="
            kubectl -n kube-system get pods -l app.kubernetes.io/name=aws-ebs-csi-driver -o wide || true
            kubectl -n kube-system describe pods -l app.kubernetes.io/name=aws-ebs-csi-driver || true
            kubectl -n kube-system logs deployment/ebs-csi-controller --all-containers --tail=200 || true
            echo "=== Recent kube-system events ==="
            kubectl -n kube-system get events --sort-by=.metadata.creationTimestamp | tail -n 80 || true
          }

          eksctl utils associate-iam-oidc-provider --cluster "${CLUSTER_NAME}" --region "${AWS_REGION}" --approve >/dev/null

          ACCOUNT_ID="$(aws sts get-caller-identity --query 'Account' --output text)"
          OIDC_PROVIDER="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --query 'cluster.identity.oidc.issuer' --output text | sed -e 's~^https://~~')"

          cat > /tmp/ebs-csi-trust-policy.json <<EOF
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {"Federated": "arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}"},
                "Action": "sts:AssumeRoleWithWebIdentity",
                "Condition": {
                  "StringEquals": {
                    "${OIDC_PROVIDER}:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa",
                    "${OIDC_PROVIDER}:aud": "sts.amazonaws.com"
                  }
                }
              }
            ]
          }
          EOF

          if aws iam get-role --role-name "${EBS_CSI_ROLE_NAME}" >/dev/null 2>&1; then
            aws iam update-assume-role-policy \
              --role-name "${EBS_CSI_ROLE_NAME}" \
              --policy-document file:///tmp/ebs-csi-trust-policy.json >/dev/null
          else
            aws iam create-role \
              --role-name "${EBS_CSI_ROLE_NAME}" \
              --assume-role-policy-document file:///tmp/ebs-csi-trust-policy.json >/dev/null
          fi

          aws iam attach-role-policy \
            --role-name "${EBS_CSI_ROLE_NAME}" \
            --policy-arn "${EBS_POLICY_ARN}" >/dev/null || true

          EBS_ROLE_ARN="arn:aws:iam::${ACCOUNT_ID}:role/${EBS_CSI_ROLE_NAME}"
          ADDON_STATUS="$(aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver --region "${AWS_REGION}" --query 'addon.status' --output text 2>/dev/null || true)"

          if [ -z "${ADDON_STATUS}" ] || [ "${ADDON_STATUS}" = "None" ]; then
            echo "Installing aws-ebs-csi-driver add-on."
            aws eks create-addon \
              --cluster-name "${CLUSTER_NAME}" \
              --addon-name aws-ebs-csi-driver \
              --region "${AWS_REGION}" \
              --service-account-role-arn "${EBS_ROLE_ARN}" \
              --resolve-conflicts OVERWRITE >/dev/null
          elif [ "${ADDON_STATUS}" = "CREATE_FAILED" ] || [ "${ADDON_STATUS}" = "DEGRADED" ] || [ "${ADDON_STATUS}" = "DELETE_FAILED" ] || [ "${ADDON_STATUS}" = "UPDATE_FAILED" ]; then
            echo "aws-ebs-csi-driver is ${ADDON_STATUS}; deleting and recreating."
            aws eks delete-addon \
              --cluster-name "${CLUSTER_NAME}" \
              --addon-name aws-ebs-csi-driver \
              --region "${AWS_REGION}" >/dev/null || true
            aws eks wait addon-deleted \
              --cluster-name "${CLUSTER_NAME}" \
              --addon-name aws-ebs-csi-driver \
              --region "${AWS_REGION}" || true
            aws eks create-addon \
              --cluster-name "${CLUSTER_NAME}" \
              --addon-name aws-ebs-csi-driver \
              --region "${AWS_REGION}" \
              --service-account-role-arn "${EBS_ROLE_ARN}" \
              --resolve-conflicts OVERWRITE >/dev/null
          else
            echo "aws-ebs-csi-driver is ${ADDON_STATUS}; updating add-on."
            aws eks update-addon \
              --cluster-name "${CLUSTER_NAME}" \
              --addon-name aws-ebs-csi-driver \
              --region "${AWS_REGION}" \
              --service-account-role-arn "${EBS_ROLE_ARN}" \
              --resolve-conflicts OVERWRITE >/dev/null
          fi

          if ! aws eks wait addon-active --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver --region "${AWS_REGION}"; then
            echo "EBS CSI add-on did not become ACTIVE."
            show_ebs_debug
            exit 1
          fi

          if ! kubectl -n kube-system rollout status deployment/ebs-csi-controller --timeout=600s; then
            echo "EBS CSI controller deployment rollout failed."
            show_ebs_debug
            exit 1
          fi

          if ! kubectl -n kube-system rollout status daemonset/ebs-csi-node --timeout=600s; then
            echo "EBS CSI node daemonset rollout failed."
            show_ebs_debug
            exit 1
          fi

      - name: Resolve image refs (build only when code changed)
        id: images
        env:
          APP_CHANGED: ${{ needs.changes.outputs.app }}
          AUTH_CHANGED: ${{ needs.changes.outputs.auth }}
        run: |
          set -euo pipefail

          ENV_NAME="${{ steps.vars.outputs.env_name }}"
          ACCOUNT_ID="$(aws sts get-caller-identity --query 'Account' --output text)"
          REGISTRY="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"

          APPOINTMENTS_REPO="cloud-projet-${ENV_NAME}/appointments-api"
          AUTH_REPO="cloud-projet-${ENV_NAME}/auth-api"

          APPOINTMENTS_TAG="${GITHUB_SHA}"
          AUTH_TAG="${GITHUB_SHA}"
          BUILD_APPOINTMENTS=false
          BUILD_AUTH=false

          if [ "${GITHUB_EVENT_NAME}" = "workflow_dispatch" ] || [ "${APP_CHANGED}" = "true" ]; then
            BUILD_APPOINTMENTS=true
          else
            APPOINTMENTS_TAG="latest"
          fi

          if [ "${GITHUB_EVENT_NAME}" = "workflow_dispatch" ] || [ "${AUTH_CHANGED}" = "true" ]; then
            BUILD_AUTH=true
          else
            AUTH_TAG="latest"
          fi

          BUILD_ANY=false
          if [ "${BUILD_APPOINTMENTS}" = "true" ] || [ "${BUILD_AUTH}" = "true" ]; then
            BUILD_ANY=true
          fi

          echo "registry=${REGISTRY}" >> "$GITHUB_OUTPUT"
          echo "appointments_repo=${APPOINTMENTS_REPO}" >> "$GITHUB_OUTPUT"
          echo "auth_repo=${AUTH_REPO}" >> "$GITHUB_OUTPUT"
          echo "appointments_tag=${APPOINTMENTS_TAG}" >> "$GITHUB_OUTPUT"
          echo "auth_tag=${AUTH_TAG}" >> "$GITHUB_OUTPUT"
          echo "build_appointments=${BUILD_APPOINTMENTS}" >> "$GITHUB_OUTPUT"
          echo "build_auth=${BUILD_AUTH}" >> "$GITHUB_OUTPUT"
          echo "build_any=${BUILD_ANY}" >> "$GITHUB_OUTPUT"
          echo "appointments_image=${REGISTRY}/${APPOINTMENTS_REPO}:${APPOINTMENTS_TAG}" >> "$GITHUB_OUTPUT"
          echo "auth_image=${REGISTRY}/${AUTH_REPO}:${AUTH_TAG}" >> "$GITHUB_OUTPUT"

      - name: Ensure ECR repositories exist
        run: |
          set -euo pipefail
          for repo in "${{ steps.images.outputs.appointments_repo }}" "${{ steps.images.outputs.auth_repo }}"; do
            if aws ecr describe-repositories --repository-names "${repo}" --region "${AWS_REGION}" >/dev/null 2>&1; then
              echo "ECR repository exists: ${repo}"
            else
              echo "Creating ECR repository: ${repo}"
              aws ecr create-repository --repository-name "${repo}" --region "${AWS_REGION}" >/dev/null
            fi
          done

      - name: Login to ECR
        if: steps.images.outputs.build_any == 'true'
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        if: steps.images.outputs.build_any == 'true'
        uses: docker/setup-buildx-action@v3

      - name: Build and push appointments image
        if: steps.images.outputs.build_appointments == 'true'
        uses: docker/build-push-action@v6
        with:
          context: ./app
          file: ./app/Dockerfile
          push: true
          tags: |
            ${{ steps.images.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:${{ github.sha }}
            ${{ steps.images.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/appointments-api:latest
          cache-from: type=gha,scope=appointments-local
          cache-to: type=gha,mode=max,scope=appointments-local

      - name: Build and push auth image
        if: steps.images.outputs.build_auth == 'true'
        uses: docker/build-push-action@v6
        with:
          context: ./auth-service
          file: ./auth-service/Dockerfile
          push: true
          tags: |
            ${{ steps.images.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:${{ github.sha }}
            ${{ steps.images.outputs.registry }}/cloud-projet-${{ steps.vars.outputs.env_name }}/auth-api:latest
          cache-from: type=gha,scope=auth-local
          cache-to: type=gha,mode=max,scope=auth-local

      - name: Verify required images exist in ECR
        run: |
          set -euo pipefail

          verify_image() {
            local repo="$1"
            local tag="$2"
            if aws ecr describe-images --repository-name "${repo}" --image-ids "imageTag=${tag}" --region "${AWS_REGION}" >/dev/null 2>&1; then
              echo "Image present: ${repo}:${tag}"
            else
              echo "Missing image in ECR: ${repo}:${tag}"
              echo "If this was a k8s-only change, ensure a previous successful run pushed the :latest tag."
              exit 1
            fi
          }

          verify_image "${{ steps.images.outputs.appointments_repo }}" "${{ steps.images.outputs.appointments_tag }}"
          verify_image "${{ steps.images.outputs.auth_repo }}" "${{ steps.images.outputs.auth_tag }}"

      - name: Render manifests
        run: |
          set -euo pipefail
          escape_for_sed() {
            printf '%s' "$1" | sed -e 's/[\/&|]/\\&/g'
          }

          DB_PASSWORD_ESCAPED="$(escape_for_sed "${{ steps.vars.outputs.db_password }}")"
          AUTH_JWT_SECRET_ESCAPED="$(escape_for_sed "${{ steps.vars.outputs.auth_jwt_secret }}")"

          rm -rf rendered
          cp -R k8s/eks rendered
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s/namespace: demo/namespace: ${{ steps.vars.outputs.namespace }}/g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__APPOINTMENTS_IMAGE__|${{ steps.images.outputs.appointments_image }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__AUTH_IMAGE__|${{ steps.images.outputs.auth_image }}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__SET_IN_CLUSTER__|${DB_PASSWORD_ESCAPED}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__AUTH_JWT_SECRET__|${AUTH_JWT_SECRET_ESCAPED}|g"
          find rendered -type f -name "*.yaml" -print0 | xargs -0 sed -i "s|__APPOINTMENTS_BASE_URL__|http://placeholder.local|g"
          if [ "${{ steps.vars.outputs.env_name }}" = "test" ]; then
            sed -i 's/replicas: 3/replicas: 1/g' rendered/07-api-deployment.yaml
            sed -i 's/replicas: 3/replicas: 1/g' rendered/11-auth-deployment.yaml
            sed -i 's/minReplicas: 2/minReplicas: 1/g' rendered/14-api-hpa.yaml
            sed -i 's/minReplicas: 2/minReplicas: 1/g' rendered/15-auth-hpa.yaml
          fi

      - name: Validate rendered manifests
        run: |
          set -euo pipefail

          unresolved="$(grep -R -n \"__APPOINTMENTS_IMAGE__\\|__AUTH_IMAGE__\\|__SET_IN_CLUSTER__\\|__AUTH_JWT_SECRET__\" rendered || true)"
          if [ -n "${unresolved}" ]; then
            echo "Found unresolved placeholders in rendered manifests:"
            echo "${unresolved}" | head -n 50
            exit 1
          fi

          demo_ns="$(grep -R -n \"namespace: demo\" rendered || true)"
          if [ -n "${demo_ns}" ]; then
            echo "Found unrendered namespace 'demo' in manifests:"
            echo "${demo_ns}" | head -n 50
            exit 1
          fi

          kubectl kustomize rendered >/dev/null
          kubectl kustomize rendered | kubectl apply --dry-run=client -f - >/dev/null

      - name: Apply and wait for postgres first
        run: |
          set -euo pipefail
          NS="${{ steps.vars.outputs.namespace }}"
          SC_NAME="cloud-projet-gp3"

          show_debug() {
            echo "=== Postgres deployment (${NS}) ==="
            kubectl get deployment/postgres -n "${NS}" -o wide || true
            kubectl describe deployment/postgres -n "${NS}" || true
            echo "=== Postgres PVC (${NS}) ==="
            kubectl get pvc/postgres-data -n "${NS}" -o wide || true
            kubectl describe pvc/postgres-data -n "${NS}" || true
            echo "=== Postgres pods (${NS}) ==="
            kubectl get pods -n "${NS}" -l app=postgres -o wide || true
            kubectl describe pods -n "${NS}" -l app=postgres || true
            echo "=== Postgres logs (${NS}) ==="
            kubectl logs -n "${NS}" -l app=postgres --all-containers --tail=200 || true
            echo "=== Recent events (${NS}) ==="
            kubectl get events -n "${NS}" --sort-by=.metadata.creationTimestamp | tail -n 80 || true
            echo "=== StorageClass (${SC_NAME}) ==="
            kubectl get storageclass "${SC_NAME}" -o yaml || true
            echo "=== EBS CSI status (kube-system) ==="
            kubectl get deployment ebs-csi-controller -n kube-system -o wide || true
            kubectl describe deployment ebs-csi-controller -n kube-system || true
            kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver -o wide || true
            kubectl logs -n kube-system deployment/ebs-csi-controller --all-containers --tail=200 || true
          }

          kubectl get namespace "${NS}" >/dev/null

          if kubectl get storageclass "${SC_NAME}" >/dev/null 2>&1; then
            PROVISIONER="$(kubectl get storageclass "${SC_NAME}" -o jsonpath='{.provisioner}' || true)"
            if [ "${PROVISIONER}" != "ebs.csi.aws.com" ]; then
              echo "StorageClass ${SC_NAME} has provisioner '${PROVISIONER}', expected 'ebs.csi.aws.com'."
              kubectl get storageclass "${SC_NAME}" -o yaml || true
              exit 1
            fi
          else
            echo "Creating StorageClass ${SC_NAME} for EBS CSI."
            cat <<'EOF' | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: cloud-projet-gp3
          provisioner: ebs.csi.aws.com
          volumeBindingMode: WaitForFirstConsumer
          parameters:
            type: gp3
            csi.storage.k8s.io/fstype: ext4
          allowVolumeExpansion: true
          EOF
          fi

          if ! grep -q "storageClassName: ${SC_NAME}" rendered/02-postgres-pvc.yaml; then
            echo "rendered/02-postgres-pvc.yaml must use storageClassName: ${SC_NAME}"
            grep -n "storageClassName" rendered/02-postgres-pvc.yaml || true
            exit 1
          fi

          if kubectl get pvc/postgres-data -n "${NS}" >/dev/null 2>&1; then
            PVC_PHASE="$(kubectl get pvc/postgres-data -n "${NS}" -o jsonpath='{.status.phase}' || true)"
            PVC_VOLUME="$(kubectl get pvc/postgres-data -n "${NS}" -o jsonpath='{.spec.volumeName}' || true)"
            if [ "${PVC_PHASE}" = "Pending" ] && [ -z "${PVC_VOLUME}" ]; then
              echo "Deleting stale unbound pvc/postgres-data before re-apply."
              kubectl delete pvc/postgres-data -n "${NS}" --wait=true || true
            fi
          fi
          kubectl apply -f rendered/01-postgres-secret.yaml
          kubectl apply -f rendered/02-postgres-pvc.yaml
          kubectl apply -f rendered/04-postgres-service.yaml
          kubectl apply -f rendered/03-postgres-deployment.yaml

          echo "Waiting for pvc/postgres-data to bind and Postgres to become Ready..."
          end=$(( $(date +%s) + 600 ))
          last_report=0
          while [ "$(date +%s)" -lt "${end}" ]; do
            pvc_phase="$(kubectl get pvc/postgres-data -n "${NS}" -o jsonpath='{.status.phase}' 2>/dev/null || true)"
            pod_name="$(kubectl get pods -n "${NS}" -l app=postgres -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"

            ready=""
            if [ -n "${pod_name}" ]; then
              ready="$(kubectl get pod "${pod_name}" -n "${NS}" -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || true)"

              waiting_reason="$(kubectl get pod "${pod_name}" -n "${NS}" -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}' 2>/dev/null || true)"
              waiting_msg="$(kubectl get pod "${pod_name}" -n "${NS}" -o jsonpath='{.status.containerStatuses[0].state.waiting.message}' 2>/dev/null || true)"
              if [ -n "${waiting_reason}" ]; then
                case "${waiting_reason}" in
                  ErrImagePull|ImagePullBackOff|CreateContainerConfigError|CreateContainerError)
                    echo "Postgres pod is failing to start: ${waiting_reason}"
                    echo "${waiting_msg}"
                    show_debug
                    exit 1
                    ;;
                  CrashLoopBackOff)
                    echo "Postgres container is crashlooping."
                    show_debug
                    exit 1
                    ;;
                esac
              fi

              sched_status="$(kubectl get pod "${pod_name}" -n "${NS}" -o jsonpath='{.status.conditions[?(@.type=="PodScheduled")].status}' 2>/dev/null || true)"
              if [ "${sched_status}" = "False" ]; then
                sched_msg="$(kubectl get pod "${pod_name}" -n "${NS}" -o jsonpath='{.status.conditions[?(@.type=="PodScheduled")].message}' 2>/dev/null || true)"
                echo "Postgres pod is unschedulable: ${sched_msg}"
                show_debug
                exit 1
              fi
            fi

            now="$(date +%s)"
            if [ "$(( now - last_report ))" -ge 60 ]; then
              echo "Status: pvc=${pvc_phase:-unknown} pod=${pod_name:-none} ready=${ready:-unknown}"
              last_report="${now}"
            fi

            if [ "${pvc_phase}" = "Bound" ] && [ "${ready}" = "True" ]; then
              echo "Postgres is Ready."
              exit 0
            fi

            sleep 10
          done

          echo "Timed out waiting for Postgres readiness."
          show_debug
          exit 1

      - name: Apply remaining manifests
        run: kubectl apply -k rendered

      - name: Wait ingress and patch auth base URL
        run: |
          set -euo pipefail
          NS="${{ steps.vars.outputs.namespace }}"

          show_debug() {
            echo "=== Cluster nodes ==="
            kubectl get nodes -o wide || true
            echo "=== Namespace resources (${NS}) ==="
            kubectl get all -n "${NS}" -o wide || true
            echo "=== Ingress describe (${NS}) ==="
            kubectl describe ingress app-ingress -n "${NS}" || true
            echo "=== Recent namespace events (${NS}) ==="
            kubectl get events -n "${NS}" --sort-by=.metadata.creationTimestamp | tail -n 80 || true
            echo "=== AWS LB controller status/logs ==="
            kubectl get deployment aws-load-balancer-controller -n kube-system -o wide || true
            kubectl logs -n kube-system deployment/aws-load-balancer-controller --tail=200 || true
            echo "=== Appointments describe ==="
            kubectl describe deployment/appointments-api -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=appointments-api || true
            echo "=== Appointments logs ==="
            kubectl logs -n "${NS}" -l app=appointments-api --all-containers --tail=200 || true
            echo "=== Auth describe ==="
            kubectl describe deployment/auth-api -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=auth-api || true
            echo "=== Auth logs ==="
            kubectl logs -n "${NS}" -l app=auth-api --all-containers --tail=200 || true
            echo "=== Postgres describe/logs ==="
            kubectl describe deployment/postgres -n "${NS}" || true
            kubectl describe pods -n "${NS}" -l app=postgres || true
            kubectl logs -n "${NS}" -l app=postgres --all-containers --tail=200 || true
          }

          kubectl rollout status deployment/postgres -n "${NS}" --timeout=1200s || { show_debug; exit 1; }
          kubectl rollout status deployment/appointments-api -n "${NS}" --timeout=600s || { show_debug; exit 1; }
          kubectl rollout status deployment/auth-api -n "${NS}" --timeout=600s || { show_debug; exit 1; }
          for i in $(seq 1 60); do
            ALB_HOST=$(kubectl get ingress app-ingress -n "${NS}" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || true)
            if [ -n "$ALB_HOST" ]; then break; fi
            sleep 10
          done
          if [ -z "$ALB_HOST" ]; then
            echo "ALB hostname not ready yet"
            show_debug
            exit 1
          fi
          kubectl patch configmap auth-config -n "${NS}" --type merge -p "{\"data\":{\"AUTH_APP_BASE_URL\":\"http://$ALB_HOST\"}}"
          kubectl rollout restart deployment/auth-api -n "${NS}"
          echo "Application URL: http://$ALB_HOST"
